"use strict";(globalThis.webpackChunkai_robotics_textbook=globalThis.webpackChunkai_robotics_textbook||[]).push([[3601],{5301:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>u,frontMatter:()=>l,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-4-vla/index","title":"Module 4: Vision-Language-Action (VLA)","description":"This capstone module integrates all previous learning into a complete system that can understand natural language commands, perceive its environment, and execute complex tasks. You\'ll build an autonomous humanoid robot that responds to voice commands.","source":"@site/docs/module-4-vla/index.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/","permalink":"/ai-robotics-textbook/docs/module-4-vla/","draft":false,"unlisted":false,"editUrl":"https://github.com/humonide-book/ai-robotics-textbook/edit/main/docs/module-4-vla/index.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_label":"Module 4: Vision-Language-Action (VLA)","sidebar_position":5},"sidebar":"textbookSidebar","previous":{"title":"Exercises","permalink":"/ai-robotics-textbook/docs/module-4-vla/exercises"},"next":{"title":"References","permalink":"/ai-robotics-textbook/docs/references"}}');var t=i(4848),s=i(8453);const l={sidebar_label:"Module 4: Vision-Language-Action (VLA)",sidebar_position:5},r="Module 4: Vision-Language-Action (VLA)",a={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Module Overview",id:"module-overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Getting Started",id:"getting-started",level:2}];function d(e){const n={admonition:"admonition",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(n.p,{children:"This capstone module integrates all previous learning into a complete system that can understand natural language commands, perceive its environment, and execute complex tasks. You'll build an autonomous humanoid robot that responds to voice commands."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After completing this module, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Implement voice processing systems using Whisper"}),"\n",(0,t.jsx)(n.li,{children:"Design LLM-based cognitive planning that translates to ROS actions"}),"\n",(0,t.jsx)(n.li,{children:"Integrate multi-modal interaction (speech, vision, gesture)"}),"\n",(0,t.jsx)(n.li,{children:"Perform object recognition and manipulation"}),"\n",(0,t.jsx)(n.li,{children:"Execute end-to-end tasks based on voice commands"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"module-overview",children:"Module Overview"}),"\n",(0,t.jsx)(n.p,{children:"The culmination of your robotics education brings together all concepts learned. This module covers:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice Processing"}),": Using Whisper for speech-to-text conversion"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cognitive Planning"}),": LLM-based reasoning for task execution"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-Modal Integration"}),": Combining vision, language, and action"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Recognition"}),": Identifying and manipulating objects"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Capstone Project"}),": Autonomous humanoid executing voice commands"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Before starting this module, ensure you have:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completed all previous modules"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of AI/ML concepts from Module 3"}),"\n",(0,t.jsx)(n.li,{children:"Experience with ROS 2 control systems from Module 1"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,t.jsx)(n.p,{children:"This module integrates all components from previous modules into a unified system. You'll connect voice processing to your AI planning systems and execute complex tasks in simulation."}),"\n",(0,t.jsx)(n.admonition,{title:"Congratulations",type:"success",children:(0,t.jsx)(n.p,{children:"You've reached the capstone module! This represents the integration of all concepts learned throughout the textbook."})})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>r});var o=i(6540);const t={},s=o.createContext(t);function l(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);