"use strict";(globalThis.webpackChunkai_robotics_textbook=globalThis.webpackChunkai_robotics_textbook||[]).push([[4768],{4727:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>m,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-2-digital-twin/chapter-4","title":"Chapter 4: Sensor Simulation and Integration","description":"Overview","source":"@site/docs/module-2-digital-twin/chapter-4.md","sourceDirName":"module-2-digital-twin","slug":"/module-2-digital-twin/chapter-4","permalink":"/ai-robotics-textbook/docs/module-2-digital-twin/chapter-4","draft":false,"unlisted":false,"editUrl":"https://github.com/humonide-book/ai-robotics-textbook/edit/main/docs/module-2-digital-twin/chapter-4.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_label":"Chapter 4: Sensor Simulation and Integration","sidebar_position":2},"sidebar":"textbookSidebar","previous":{"title":"Chapter 3: Physics Simulation Fundamentals","permalink":"/ai-robotics-textbook/docs/module-2-digital-twin/chapter-3"},"next":{"title":"Module 2: Digital Twin (Gazebo & Unity)","permalink":"/ai-robotics-textbook/docs/module-2-digital-twin/"}}');var r=i(4848),a=i(8453);const o={sidebar_label:"Chapter 4: Sensor Simulation and Integration",sidebar_position:2},t="Chapter 4: Sensor Simulation and Integration",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"4.1 Introduction to Sensor Simulation",id:"41-introduction-to-sensor-simulation",level:2},{value:"Key Sensor Types for Humanoid Robots:",id:"key-sensor-types-for-humanoid-robots",level:3},{value:"4.2 LiDAR Sensor Simulation",id:"42-lidar-sensor-simulation",level:2},{value:"LiDAR Physics in Simulation",id:"lidar-physics-in-simulation",level:3},{value:"Gazebo LiDAR Implementation",id:"gazebo-lidar-implementation",level:3},{value:"LiDAR Parameters",id:"lidar-parameters",level:3},{value:"4.3 Depth Camera Simulation",id:"43-depth-camera-simulation",level:2},{value:"Depth Camera Principles",id:"depth-camera-principles",level:3},{value:"Gazebo Depth Camera Configuration",id:"gazebo-depth-camera-configuration",level:3},{value:"Depth Camera Parameters",id:"depth-camera-parameters",level:3},{value:"4.4 IMU Sensor Simulation",id:"44-imu-sensor-simulation",level:2},{value:"IMU Principles",id:"imu-principles",level:3},{value:"Gazebo IMU Configuration",id:"gazebo-imu-configuration",level:3},{value:"IMU Parameters",id:"imu-parameters",level:3},{value:"4.5 Unity Sensor Simulation",id:"45-unity-sensor-simulation",level:2},{value:"Unity Robotics Package",id:"unity-robotics-package",level:3},{value:"Unity Sensor Components",id:"unity-sensor-components",level:3},{value:"4.6 Sensor Integration with ROS 2",id:"46-sensor-integration-with-ros-2",level:2},{value:"ROS 2 Sensor Interfaces",id:"ros-2-sensor-interfaces",level:3},{value:"Sensor Data Processing Pipeline",id:"sensor-data-processing-pipeline",level:3},{value:"4.7 Sensor Validation and Calibration",id:"47-sensor-validation-and-calibration",level:2},{value:"Validation Techniques",id:"validation-techniques",level:3},{value:"Calibration Procedures",id:"calibration-procedures",level:3},{value:"4.8 Practical Exercise: Sensor Integration",id:"48-practical-exercise-sensor-integration",level:2},{value:"Exercise Objective",id:"exercise-objective",level:3},{value:"Steps",id:"steps",level:3},{value:"Expected Results",id:"expected-results",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Terms",id:"key-terms",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-4-sensor-simulation-and-integration",children:"Chapter 4: Sensor Simulation and Integration"})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"This chapter focuses on simulating various sensors for humanoid robots, including LiDAR, depth cameras, and IMU sensors. Students will learn to implement sensor simulation, configure sensor properties, and integrate sensors into simulation environments for realistic testing."}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"After completing this chapter, students will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Implement LiDAR sensor simulation in Gazebo"}),"\n",(0,r.jsx)(n.li,{children:"Configure depth camera simulation with realistic parameters"}),"\n",(0,r.jsx)(n.li,{children:"Integrate IMU sensor simulation for orientation and acceleration"}),"\n",(0,r.jsx)(n.li,{children:"Validate sensor data accuracy and noise characteristics"}),"\n",(0,r.jsx)(n.li,{children:"Create sensor fusion systems in simulation"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"41-introduction-to-sensor-simulation",children:"4.1 Introduction to Sensor Simulation"}),"\n",(0,r.jsx)(n.p,{children:"Sensor simulation is crucial for creating realistic digital twins that accurately represent how robots perceive their environment. In simulation, sensors must behave as closely as possible to their real-world counterparts, including appropriate noise models, latency, and accuracy limitations."}),"\n",(0,r.jsx)(n.h3,{id:"key-sensor-types-for-humanoid-robots",children:"Key Sensor Types for Humanoid Robots:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LiDAR"}),": Light Detection and Ranging for 3D environment mapping"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth Cameras"}),": RGB-D sensors for 3D scene understanding"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMU"}),": Inertial Measurement Unit for orientation and acceleration"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Force/Torque Sensors"}),": For contact detection and manipulation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Encoders"}),": For joint position feedback"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"42-lidar-sensor-simulation",children:"4.2 LiDAR Sensor Simulation"}),"\n",(0,r.jsx)(n.h3,{id:"lidar-physics-in-simulation",children:"LiDAR Physics in Simulation"}),"\n",(0,r.jsx)(n.p,{children:"LiDAR sensors emit laser beams and measure the time-of-flight to determine distances. In simulation, this is modeled by ray tracing algorithms that detect intersections with objects in the environment."}),"\n",(0,r.jsx)(n.h3,{id:"gazebo-lidar-implementation",children:"Gazebo LiDAR Implementation"}),"\n",(0,r.jsx)(n.p,{children:"Here's an example of a LiDAR sensor configuration in a URDF model:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="lidar_link">\n  <sensor name="lidar_sensor" type="ray">\n    <pose>0 0 0 0 0 0</pose>\n    <visualize>true</visualize>\n    <update_rate>10</update_rate>\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>720</samples>\n          <resolution>1</resolution>\n          <min_angle>-3.14159</min_angle>\n          <max_angle>3.14159</max_angle>\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.1</min>\n        <max>30.0</max>\n        <resolution>0.01</resolution>\n      </range>\n    </ray>\n    <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n      <ros>\n        <namespace>/lidar</namespace>\n        <remapping>~/out:=scan</remapping>\n      </ros>\n      <output_type>sensor_msgs/LaserScan</output_type>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"lidar-parameters",children:"LiDAR Parameters"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Range"}),": Minimum and maximum detectable distances"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Resolution"}),": Angular resolution of the sensor"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Update Rate"}),": How frequently the sensor publishes data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Noise Model"}),": Statistical model for sensor noise"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"43-depth-camera-simulation",children:"4.3 Depth Camera Simulation"}),"\n",(0,r.jsx)(n.h3,{id:"depth-camera-principles",children:"Depth Camera Principles"}),"\n",(0,r.jsx)(n.p,{children:"Depth cameras provide both color (RGB) and depth information for 3D scene understanding. In simulation, this involves rendering both color and depth images simultaneously."}),"\n",(0,r.jsx)(n.h3,{id:"gazebo-depth-camera-configuration",children:"Gazebo Depth Camera Configuration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="camera_link">\n  <sensor name="depth_camera" type="depth">\n    <update_rate>30</update_rate>\n    <camera name="head">\n      <horizontal_fov>1.047</horizontal_fov>\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>100</far>\n      </clip>\n    </camera>\n    <plugin name="camera_controller" filename="libgazebo_ros_openni_kinect.so">\n      <alwaysOn>true</alwaysOn>\n      <updateRate>10.0</updateRate>\n      <cameraName>depth_camera</cameraName>\n      <imageTopicName>/rgb/image_raw</imageTopicName>\n      <depthImageTopicName>/depth/image_raw</depthImageTopicName>\n      <pointCloudTopicName>/depth/points</pointCloudTopicName>\n      <cameraInfoTopicName>/rgb/camera_info</cameraInfoTopicName>\n      <depthImageCameraInfoTopicName>/depth/camera_info</depthImageCameraInfoTopicName>\n      <frameName>camera_depth_optical_frame</frameName>\n      <baseline>0.1</baseline>\n      <distortion_k1>0.0</distortion_k1>\n      <distortion_k2>0.0</distortion_k2>\n      <distortion_k3>0.0</distortion_k3>\n      <distortion_t1>0.0</distortion_t1>\n      <distortion_t2>0.0</distortion_t2>\n      <pointCloudCutoff>0.4</pointCloudCutoff>\n      <pointCloudCutoffMax>3.0</pointCloudCutoffMax>\n      <CxPrime>0.0</CxPrime>\n      <Cx>0.0</Cx>\n      <Cy>0.0</Cy>\n      <focalLength>0.0</focalLength>\n      <hackBaseline>0.0</hackBaseline>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"depth-camera-parameters",children:"Depth Camera Parameters"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Field of View"}),": Angular extent of the scene captured"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Resolution"}),": Image dimensions in pixels"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth Range"}),": Minimum and maximum measurable depths"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Noise Characteristics"}),": Noise models for depth accuracy"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"44-imu-sensor-simulation",children:"4.4 IMU Sensor Simulation"}),"\n",(0,r.jsx)(n.h3,{id:"imu-principles",children:"IMU Principles"}),"\n",(0,r.jsx)(n.p,{children:"IMU sensors measure linear acceleration and angular velocity, which can be integrated to estimate orientation. In humanoid robots, IMUs are crucial for balance and navigation."}),"\n",(0,r.jsx)(n.h3,{id:"gazebo-imu-configuration",children:"Gazebo IMU Configuration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<gazebo reference="imu_link">\n  <sensor name="imu_sensor" type="imu">\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0017</stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n    <plugin name="imu_plugin" filename="libgazebo_ros_imu.so">\n      <ros>\n        <namespace>/imu</namespace>\n        <remapping>~/out:=data</remapping>\n      </ros>\n      <initial_orientation_as_reference>false</initial_orientation_as_reference>\n      <body_name>imu_link</body_name>\n      <update_rate>100</update_rate>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"imu-parameters",children:"IMU Parameters"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Update Rate"}),": Frequency of sensor measurements"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Noise Characteristics"}),": Gaussian noise models for each axis"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Bias Drift"}),": Long-term drift characteristics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Scale Factor Error"}),": Multiplicative errors in measurements"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"45-unity-sensor-simulation",children:"4.5 Unity Sensor Simulation"}),"\n",(0,r.jsx)(n.h3,{id:"unity-robotics-package",children:"Unity Robotics Package"}),"\n",(0,r.jsx)(n.p,{children:"Unity provides the Unity Robotics Package for sensor simulation in robotics applications:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-csharp",children:'using Unity.Robotics.Sensors;\n\npublic class ROSTfSender : MonoBehaviour\n{\n    [SerializeField]\n    string m_FrameId;\n    public string FrameId { get { return m_FrameId; } set { m_FrameId = value; } }\n\n    [SerializeField]\n    string m_ParentFrameId = "map";\n    public string ParentFrameId { get { return m_ParentFrameId; } set { m_ParentFrameId = value; } }\n\n    [SerializeField]\n    float m_QueueSize = 10;\n    public float QueueSize { get { return m_QueueSize; } set { m_QueueSize = value; } }\n\n    [SerializeField]\n    float m_PublishRate = 100;\n    public float PublishRate { get { return m_PublishRate; } set { m_PublishRate = value; } }\n\n    // Implementation details for TF publishing\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"unity-sensor-components",children:"Unity Sensor Components"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LiDAR Sensor"}),": Raycasting-based distance measurement"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Camera Sensor"}),": RGB and depth image capture"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMU Sensor"}),": Acceleration and rotation simulation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Force/Torque Sensor"}),": Contact force measurement"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"46-sensor-integration-with-ros-2",children:"4.6 Sensor Integration with ROS 2"}),"\n",(0,r.jsx)(n.h3,{id:"ros-2-sensor-interfaces",children:"ROS 2 Sensor Interfaces"}),"\n",(0,r.jsx)(n.p,{children:"Sensors in simulation publish to ROS 2 topics following standard message types:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"sensor_msgs/LaserScan"}),": For LiDAR data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"sensor_msgs/Image"}),": For camera images"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"sensor_msgs/PointCloud2"}),": For 3D point cloud data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"sensor_msgs/Imu"}),": For IMU data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"sensor_msgs/JointState"}),": For joint positions/speeds/effort"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"sensor-data-processing-pipeline",children:"Sensor Data Processing Pipeline"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image, Imu\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass SensorProcessor(Node):\n    def __init__(self):\n        super().__init__('sensor_processor')\n        self.lidar_subscription = self.create_subscription(\n            LaserScan,\n            'lidar/scan',\n            self.lidar_callback,\n            10)\n        self.camera_subscription = self.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.camera_callback,\n            10)\n        self.imu_subscription = self.create_subscription(\n            Imu,\n            'imu/data',\n            self.imu_callback,\n            10)\n\n        self.bridge = CvBridge()\n\n    def lidar_callback(self, msg):\n        # Process LiDAR data\n        ranges = np.array(msg.ranges)\n        # Implement processing logic\n        self.get_logger().info(f'Lidar: {len(ranges)} points')\n\n    def camera_callback(self, msg):\n        # Process camera data\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n        # Implement processing logic\n        self.get_logger().info(f'Camera: {cv_image.shape}')\n\n    def imu_callback(self, msg):\n        # Process IMU data\n        orientation = msg.orientation\n        angular_velocity = msg.angular_velocity\n        linear_acceleration = msg.linear_acceleration\n        # Implement processing logic\n        self.get_logger().info(f'IMU: Orientation updated')\n"})}),"\n",(0,r.jsx)(n.h2,{id:"47-sensor-validation-and-calibration",children:"4.7 Sensor Validation and Calibration"}),"\n",(0,r.jsx)(n.h3,{id:"validation-techniques",children:"Validation Techniques"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ground Truth Comparison"}),": Compare sensor output with known values"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cross-Sensor Validation"}),": Compare readings from different sensors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Temporal Consistency"}),": Check for consistency over time"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Environmental Validation"}),": Test in various lighting/condition scenarios"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"calibration-procedures",children:"Calibration Procedures"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Intrinsic Calibration"}),": Internal sensor parameters"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Extrinsic Calibration"}),": Position and orientation relative to robot"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Temporal Calibration"}),": Synchronization between sensors"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"48-practical-exercise-sensor-integration",children:"4.8 Practical Exercise: Sensor Integration"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-objective",children:"Exercise Objective"}),"\n",(0,r.jsx)(n.p,{children:"Integrate multiple sensors into a humanoid robot model and validate their outputs."}),"\n",(0,r.jsx)(n.h3,{id:"steps",children:"Steps"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Create a humanoid robot model with LiDAR, camera, and IMU sensors"}),"\n",(0,r.jsx)(n.li,{children:"Configure sensor parameters to match real hardware specifications"}),"\n",(0,r.jsx)(n.li,{children:"Implement ROS 2 nodes to process sensor data"}),"\n",(0,r.jsx)(n.li,{children:"Validate sensor outputs in simulation environment"}),"\n",(0,r.jsx)(n.li,{children:"Test sensor fusion for environment perception"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"expected-results",children:"Expected Results"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"All sensors publish data to appropriate ROS 2 topics"}),"\n",(0,r.jsx)(n.li,{children:"Sensor data is realistic and physically plausible"}),"\n",(0,r.jsx)(n.li,{children:"Sensor fusion provides coherent environment understanding"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"Sensor simulation is essential for creating realistic digital twins that accurately represent how humanoid robots perceive their environment. Proper configuration of LiDAR, camera, and IMU sensors ensures that simulation results closely match real-world performance, enabling effective testing and development of perception systems."}),"\n",(0,r.jsx)(n.h2,{id:"key-terms",children:"Key Terms"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor Simulation"}),": Computational modeling of sensor behavior"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ray Tracing"}),": Technique for simulating light/laser interactions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Noise Model"}),": Statistical model for sensor inaccuracies"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor Fusion"}),": Combining data from multiple sensors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ground Truth"}),": Known accurate values for validation"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"http://gazebosim.org/tutorials?tut=ros_gzplugins_sensors",children:"Gazebo Sensor Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/Unity-Technologies/Unity-Robotics-Hub",children:"Unity Robotics Package"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/rolling/p/sensor_msgs/",children:"ROS 2 Sensor Messages"})}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>t});var s=i(6540);const r={},a=s.createContext(r);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);