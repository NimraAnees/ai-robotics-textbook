"use strict";(globalThis.webpackChunkai_robotics_textbook=globalThis.webpackChunkai_robotics_textbook||[]).push([[3431],{8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var r=i(6540);const s={},t=r.createContext(s);function o(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(t.Provider,{value:n},e.children)}},8938:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"module-4-vla/chapter-7","title":"Chapter 7: Voice Processing with Whisper","description":"Overview","source":"@site/docs/module-4-vla/chapter-7.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-7","permalink":"/ai-robotics-textbook/docs/module-4-vla/chapter-7","draft":false,"unlisted":false,"editUrl":"https://github.com/humonide-book/ai-robotics-textbook/edit/main/docs/module-4-vla/chapter-7.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_label":"Chapter 7: Voice Processing with Whisper","sidebar_position":1},"sidebar":"textbookSidebar","previous":{"title":"Learning Objectives","permalink":"/ai-robotics-textbook/docs/module-4-vla/learning-objectives"},"next":{"title":"Chapter 8: LLM-Based Cognitive Planning to ROS Actions","permalink":"/ai-robotics-textbook/docs/module-4-vla/chapter-8"}}');var s=i(4848),t=i(8453);const o={sidebar_label:"Chapter 7: Voice Processing with Whisper",sidebar_position:1},a="Chapter 7: Voice Processing with Whisper",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"7.1 Introduction to Whisper for Robotics",id:"71-introduction-to-whisper-for-robotics",level:2},{value:"What is Whisper?",id:"what-is-whisper",level:3},{value:"Key Features for Robotics",id:"key-features-for-robotics",level:3},{value:"Whisper Model Variants",id:"whisper-model-variants",level:3},{value:"7.2 Installing and Setting Up Whisper",id:"72-installing-and-setting-up-whisper",level:2},{value:"System Requirements",id:"system-requirements",level:3},{value:"Installation Process",id:"installation-process",level:3},{value:"Basic Whisper Usage Example",id:"basic-whisper-usage-example",level:3},{value:"7.3 Whisper Integration with ROS 2",id:"73-whisper-integration-with-ros-2",level:2},{value:"Audio Input from Microphone",id:"audio-input-from-microphone",level:3},{value:"7.4 Voice Activity Detection",id:"74-voice-activity-detection",level:2},{value:"Importance in Robotics",id:"importance-in-robotics",level:3},{value:"Implementation with Silero VAD",id:"implementation-with-silero-vad",level:3},{value:"Integration with Whisper Processing",id:"integration-with-whisper-processing",level:3},{value:"7.5 Real-time Processing Optimization",id:"75-real-time-processing-optimization",level:2},{value:"Batch Processing",id:"batch-processing",level:3},{value:"7.6 Handling Acoustic Conditions",id:"76-handling-acoustic-conditions",level:2},{value:"Noise Reduction",id:"noise-reduction",level:3},{value:"7.7 Voice Command Parsing",id:"77-voice-command-parsing",level:2},{value:"Simple Command Recognition",id:"simple-command-recognition",level:3},{value:"7.8 Practical Exercise: Voice Processing System",id:"78-practical-exercise-voice-processing-system",level:2},{value:"Exercise Objective",id:"exercise-objective",level:3},{value:"Steps",id:"steps",level:3},{value:"Expected Results",id:"expected-results",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Terms",id:"key-terms",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-7-voice-processing-with-whisper",children:"Chapter 7: Voice Processing with Whisper"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This chapter covers the implementation of voice processing systems using OpenAI's Whisper for speech-to-text conversion in humanoid robotics applications. Students will learn to integrate Whisper with ROS 2, process voice commands in real-time, and handle various acoustic conditions that may be encountered in real-world robotic environments."}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"After completing this chapter, students will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Install and configure OpenAI Whisper for robotic applications"}),"\n",(0,s.jsx)(n.li,{children:"Integrate Whisper with ROS 2 for real-time voice processing"}),"\n",(0,s.jsx)(n.li,{children:"Process voice commands and convert them to text"}),"\n",(0,s.jsx)(n.li,{children:"Handle various acoustic conditions and noise environments"}),"\n",(0,s.jsx)(n.li,{children:"Implement voice activity detection and command parsing"}),"\n",(0,s.jsx)(n.li,{children:"Validate voice processing accuracy and latency"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"71-introduction-to-whisper-for-robotics",children:"7.1 Introduction to Whisper for Robotics"}),"\n",(0,s.jsx)(n.h3,{id:"what-is-whisper",children:"What is Whisper?"}),"\n",(0,s.jsx)(n.p,{children:"Whisper is a general-purpose speech recognition model developed by OpenAI. It is designed to be robust across various domains, languages, and acoustic conditions. For robotics applications, Whisper provides:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"High accuracy speech-to-text conversion"}),"\n",(0,s.jsx)(n.li,{children:"Support for multiple languages"}),"\n",(0,s.jsx)(n.li,{children:"Robustness to background noise"}),"\n",(0,s.jsx)(n.li,{children:"Real-time processing capabilities"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"key-features-for-robotics",children:"Key Features for Robotics"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-language Support"}),": Supports 99 languages for international applications"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robustness"}),": Performs well in noisy environments"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Timestamps"}),": Provides word-level timing information"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Punctuation"}),": Automatically adds punctuation to transcriptions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Speaker Identification"}),": Can distinguish between different speakers"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"whisper-model-variants",children:"Whisper Model Variants"}),"\n",(0,s.jsx)(n.p,{children:"Whisper comes in several sizes optimized for different performance requirements:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Parameters"}),(0,s.jsx)(n.th,{children:"Required VRAM"}),(0,s.jsx)(n.th,{children:"Relative Speed"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"tiny"}),(0,s.jsx)(n.td,{children:"39 M"}),(0,s.jsx)(n.td,{children:"~1 GB"}),(0,s.jsx)(n.td,{children:"32x"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"base"}),(0,s.jsx)(n.td,{children:"74 M"}),(0,s.jsx)(n.td,{children:"~1 GB"}),(0,s.jsx)(n.td,{children:"16x"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"small"}),(0,s.jsx)(n.td,{children:"244 M"}),(0,s.jsx)(n.td,{children:"~2 GB"}),(0,s.jsx)(n.td,{children:"6x"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"medium"}),(0,s.jsx)(n.td,{children:"769 M"}),(0,s.jsx)(n.td,{children:"~5 GB"}),(0,s.jsx)(n.td,{children:"2x"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"large"}),(0,s.jsx)(n.td,{children:"1550 M"}),(0,s.jsx)(n.td,{children:"~10 GB"}),(0,s.jsx)(n.td,{children:"1x"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:'For robotics applications, the "small" or "medium" models typically provide the best balance of accuracy and computational requirements.'}),"\n",(0,s.jsx)(n.h2,{id:"72-installing-and-setting-up-whisper",children:"7.2 Installing and Setting Up Whisper"}),"\n",(0,s.jsx)(n.h3,{id:"system-requirements",children:"System Requirements"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Python 3.9 or higher"}),"\n",(0,s.jsx)(n.li,{children:"PyTorch 1.10 or higher"}),"\n",(0,s.jsx)(n.li,{children:"At least 2GB RAM (for small model) or 5GB (for medium model)"}),"\n",(0,s.jsx)(n.li,{children:"CUDA-compatible GPU recommended for real-time performance"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"installation-process",children:"Installation Process"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Install Whisper and its dependencies\npip install openai-whisper\n\n# For GPU acceleration (if available)\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# Additional dependencies for audio processing\npip install pyaudio soundfile librosa\n"})}),"\n",(0,s.jsx)(n.h3,{id:"basic-whisper-usage-example",children:"Basic Whisper Usage Example"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import whisper\n\n# Load model (downloads automatically on first use)\nmodel = whisper.load_model("small")\n\n# Transcribe audio file\nresult = model.transcribe("audio_file.wav")\n\n# Print the transcribed text\nprint(result["text"])\n'})}),"\n",(0,s.jsx)(n.h2,{id:"73-whisper-integration-with-ros-2",children:"7.3 Whisper Integration with ROS 2"}),"\n",(0,s.jsx)(n.h3,{id:"audio-input-from-microphone",children:"Audio Input from Microphone"}),"\n",(0,s.jsx)(n.p,{children:"For real-time voice processing, we need to capture audio from a microphone and process it in chunks:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nimport pyaudio\nimport numpy as np\nimport whisper\nimport threading\nfrom std_msgs.msg import String\nfrom audio_common_msgs.msg import AudioData\n\nclass WhisperNode(Node):\n    def __init__(self):\n        super().__init__(\'whisper_node\')\n\n        # Initialize Whisper model\n        self.model = whisper.load_model("small", device="cuda" if torch.cuda.is_available() else "cpu")\n\n        # Audio parameters\n        self.rate = 16000  # Sample rate\n        self.chunk = 1024  # Buffer size\n        self.format = pyaudio.paInt16\n        self.channels = 1\n\n        # Initialize PyAudio\n        self.audio = pyaudio.PyAudio()\n\n        # ROS 2 publishers\n        self.transcript_pub = self.create_publisher(String, \'voice_transcript\', 10)\n        self.command_pub = self.create_publisher(String, \'voice_command\', 10)\n\n        # Start audio stream\n        self.stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk\n        )\n\n        # Start processing thread\n        self.processing_thread = threading.Thread(target=self.process_audio)\n        self.processing_thread.daemon = True\n        self.processing_thread.start()\n\n        self.get_logger().info(\'Whisper node initialized\')\n\n    def process_audio(self):\n        """Process audio in chunks for real-time transcription"""\n        buffer = np.array([], dtype=np.float32)\n\n        while rclpy.ok():\n            # Read audio data\n            data = self.stream.read(self.chunk, exception_on_overflow=False)\n            audio_data = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\n\n            # Add to buffer\n            buffer = np.concatenate([buffer, audio_data])\n\n            # Process when buffer reaches 5 seconds (80,000 samples at 16kHz)\n            if len(buffer) >= self.rate * 5:\n                # Process audio with Whisper\n                transcript = self.transcribe_audio(buffer)\n\n                if transcript.strip():  # Only publish if there\'s actual text\n                    self.publish_transcript(transcript)\n\n                # Keep last 1 second of audio to maintain context\n                buffer = buffer[-self.rate:]\n\n    def transcribe_audio(self, audio_buffer):\n        """Transcribe audio buffer using Whisper"""\n        try:\n            # Convert to the format expected by Whisper\n            audio_tensor = torch.from_numpy(audio_buffer).to(self.model.device)\n\n            # Transcribe\n            result = self.model.transcribe(audio_tensor.cpu().numpy())\n            return result["text"]\n        except Exception as e:\n            self.get_logger().error(f\'Error during transcription: {e}\')\n            return ""\n\n    def publish_transcript(self, transcript):\n        """Publish transcript to ROS topics"""\n        # Publish raw transcript\n        transcript_msg = String()\n        transcript_msg.data = transcript\n        self.transcript_pub.publish(transcript_msg)\n\n        # Extract and publish commands (simplified - in practice you\'d use NLP)\n        command = self.extract_command(transcript)\n        if command:\n            command_msg = String()\n            command_msg.data = command\n            self.command_pub.publish(command_msg)\n\n    def extract_command(self, transcript):\n        """Simple command extraction (in practice, use more sophisticated NLP)"""\n        # Convert to lowercase for easier matching\n        text = transcript.lower().strip()\n\n        # Define simple command patterns\n        commands = [\n            "move forward", "move backward", "turn left", "turn right",\n            "stop", "sit down", "stand up", "wave", "dance", "hello"\n        ]\n\n        for cmd in commands:\n            if cmd in text:\n                return cmd\n\n        return None\n\ndef main(args=None):\n    rclpy.init(args=args)\n    whisper_node = WhisperNode()\n\n    try:\n        rclpy.spin(whisper_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        whisper_node.stream.stop_stream()\n        whisper_node.stream.close()\n        whisper_node.audio.terminate()\n        whisper_node.destroy_node()\n        rclpy.shutdown()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"74-voice-activity-detection",children:"7.4 Voice Activity Detection"}),"\n",(0,s.jsx)(n.h3,{id:"importance-in-robotics",children:"Importance in Robotics"}),"\n",(0,s.jsx)(n.p,{children:"Voice activity detection (VAD) is crucial for robotics applications to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Reduce computational load by only processing when speech is detected"}),"\n",(0,s.jsx)(n.li,{children:"Improve accuracy by avoiding background noise processing"}),"\n",(0,s.jsx)(n.li,{children:"Enable more responsive voice interaction"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"implementation-with-silero-vad",children:"Implementation with Silero VAD"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import torch\nimport torchaudio\nfrom scipy import signal\n\nclass VoiceActivityDetector:\n    def __init__(self, threshold=0.5):\n        # Load Silero VAD model\n        self.model, _ = torch.hub.load(\n            repo_or_dir=\'snakers4/silero-vad\',\n            model=\'silero_vad\',\n            force_reload=False\n        )\n        self.threshold = threshold\n        self.sample_rate = 16000\n\n    def is_speech(self, audio_chunk):\n        """Detect if audio chunk contains speech"""\n        # Ensure audio is in the right format\n        if len(audio_chunk.shape) == 1:\n            audio_chunk = audio_chunk.unsqueeze(0)\n\n        # Get VAD probability\n        vad_prob = self.model(audio_chunk, self.sample_rate).item()\n\n        return vad_prob > self.threshold\n'})}),"\n",(0,s.jsx)(n.h3,{id:"integration-with-whisper-processing",children:"Integration with Whisper Processing"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class WhisperWithVAD(WhisperNode):\n    def __init__(self):\n        super().__init__()\n\n        # Initialize VAD\n        self.vad = VoiceActivityDetector()\n\n        # Voice activity state\n        self.is_listening = False\n        self.voice_buffer = np.array([], dtype=np.float32)\n        self.silence_counter = 0\n        self.min_voice_duration = self.rate * 0.5  # Minimum 0.5 seconds of voice\n        self.max_silence_duration = self.rate * 1.0  # Maximum 1 second of silence\n\n    def process_audio(self):\n        """Process audio with VAD for more efficient processing"""\n        while rclpy.ok():\n            # Read audio data\n            data = self.stream.read(self.chunk, exception_on_overflow=False)\n            audio_data = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\n\n            # Check for voice activity\n            is_speech = self.vad.is_speech(torch.from_numpy(audio_data))\n\n            if is_speech:\n                # Add to voice buffer\n                self.voice_buffer = np.concatenate([self.voice_buffer, audio_data])\n                self.silence_counter = 0\n                self.is_listening = True\n            elif self.is_listening:\n                # We were listening but now there\'s silence\n                self.silence_counter += len(audio_data)\n\n                # If enough silence, process the collected voice\n                if self.silence_counter >= self.max_silence_duration or len(self.voice_buffer) >= self.rate * 5:\n                    if len(self.voice_buffer) >= self.min_voice_duration:\n                        # Process collected voice segment\n                        transcript = self.transcribe_audio(self.voice_buffer)\n                        if transcript.strip():\n                            self.publish_transcript(transcript)\n\n                    # Reset for next voice segment\n                    self.voice_buffer = np.array([], dtype=np.float32)\n                    self.is_listening = False\n                    self.silence_counter = 0\n'})}),"\n",(0,s.jsx)(n.h2,{id:"75-real-time-processing-optimization",children:"7.5 Real-time Processing Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"batch-processing",children:"Batch Processing"}),"\n",(0,s.jsx)(n.p,{children:"For improved efficiency, process multiple audio chunks together:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class OptimizedWhisperNode(WhisperNode):\n    def __init__(self):\n        super().__init__()\n\n        # Processing parameters\n        self.processing_interval = 2.0  # Process every 2 seconds\n        self.buffer_size = int(self.rate * self.processing_interval)\n        self.audio_buffer = np.array([], dtype=np.float32)\n\n        # Timer for periodic processing\n        self.process_timer = self.create_timer(\n            self.processing_interval,\n            self.process_buffered_audio\n        )\n\n    def process_buffered_audio(self):\n        """Process accumulated audio buffer"""\n        if len(self.audio_buffer) > 0:\n            # Process the accumulated buffer\n            transcript = self.transcribe_audio(self.audio_buffer)\n\n            if transcript.strip():\n                self.publish_transcript(transcript)\n\n            # Clear buffer\n            self.audio_buffer = np.array([], dtype=np.float32)\n\n    def process_audio(self):\n        """Accumulate audio for batch processing"""\n        while rclpy.ok():\n            data = self.stream.read(self.chunk, exception_on_overflow=False)\n            audio_data = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\n\n            # Add to buffer\n            self.audio_buffer = np.concatenate([self.audio_buffer, audio_data])\n\n            # Keep buffer size reasonable\n            if len(self.audio_buffer) > self.buffer_size * 2:\n                self.audio_buffer = self.audio_buffer[-self.buffer_size:]\n'})}),"\n",(0,s.jsx)(n.h2,{id:"76-handling-acoustic-conditions",children:"7.6 Handling Acoustic Conditions"}),"\n",(0,s.jsx)(n.h3,{id:"noise-reduction",children:"Noise Reduction"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from scipy import signal\nimport webrtcvad\n\nclass RobustWhisperNode(WhisperNode):\n    def __init__(self):\n        super().__init__()\n\n        # Initialize noise reduction\n        self.setup_noise_reduction()\n\n    def setup_noise_reduction(self):\n        """Set up noise reduction filters"""\n        # Create a simple low-pass filter to remove high-frequency noise\n        nyquist = self.rate / 2\n        cutoff = 8000  # Hz\n        order = 6\n        b, a = signal.butter(order, cutoff / nyquist, btype=\'low\')\n        self.filter_b = b\n        self.filter_a = a\n\n    def preprocess_audio(self, audio_data):\n        """Apply noise reduction and preprocessing"""\n        # Apply low-pass filter\n        filtered_audio = signal.filtfilt(self.filter_b, self.filter_a, audio_data)\n\n        # Normalize audio\n        max_val = np.max(np.abs(filtered_audio))\n        if max_val > 0:\n            filtered_audio = filtered_audio / max_val\n\n        return filtered_audio\n\n    def process_audio(self):\n        """Process audio with preprocessing"""\n        buffer = np.array([], dtype=np.float32)\n\n        while rclpy.ok():\n            data = self.stream.read(self.chunk, exception_on_overflow=False)\n            audio_data = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\n\n            # Apply preprocessing\n            processed_audio = self.preprocess_audio(audio_data)\n\n            # Add to buffer\n            buffer = np.concatenate([buffer, processed_audio])\n\n            # Process when buffer reaches 5 seconds\n            if len(buffer) >= self.rate * 5:\n                transcript = self.transcribe_audio(buffer)\n\n                if transcript.strip():\n                    self.publish_transcript(transcript)\n\n                # Keep last 1 second\n                buffer = buffer[-self.rate:]\n'})}),"\n",(0,s.jsx)(n.h2,{id:"77-voice-command-parsing",children:"7.7 Voice Command Parsing"}),"\n",(0,s.jsx)(n.h3,{id:"simple-command-recognition",children:"Simple Command Recognition"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import re\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass VoiceCommand:\n    action: str\n    parameters: dict\n    confidence: float\n\nclass VoiceCommandParser:\n    def __init__(self):\n        # Define command patterns\n        self.command_patterns = [\n            {\n                "pattern": r"go\\s+(forward|backward|left|right|up|down)",\n                "action": "move",\n                "extractor": lambda match: {"direction": match.group(1)}\n            },\n            {\n                "pattern": r"move\\s+(forward|backward|left|right|up|down)",\n                "action": "move",\n                "extractor": lambda match: {"direction": match.group(1)}\n            },\n            {\n                "pattern": r"turn\\s+(left|right)",\n                "action": "turn",\n                "extractor": lambda match: {"direction": match.group(1)}\n            },\n            {\n                "pattern": r"go\\s+to\\s+(.+)",\n                "action": "navigate",\n                "extractor": lambda match: {"location": match.group(1).strip()}\n            },\n            {\n                "pattern": r"pick\\s+up\\s+(.+)",\n                "action": "pick",\n                "extractor": lambda match: {"object": match.group(1).strip()}\n            },\n            {\n                "pattern": r"put\\s+down|place",\n                "action": "place",\n                "extractor": lambda match: {}\n            },\n            {\n                "pattern": r"stop|halt|freeze",\n                "action": "stop",\n                "extractor": lambda match: {}\n            }\n        ]\n\n    def parse_command(self, transcript: str) -> Optional[VoiceCommand]:\n        """Parse voice transcript into structured command"""\n        transcript_lower = transcript.lower().strip()\n\n        for pattern_info in self.command_patterns:\n            match = re.search(pattern_info["pattern"], transcript_lower)\n            if match:\n                try:\n                    parameters = pattern_info["extractor"](match)\n                    return VoiceCommand(\n                        action=pattern_info["action"],\n                        parameters=parameters,\n                        confidence=0.8  # Simple confidence for now\n                    )\n                except Exception:\n                    continue\n\n        # If no specific pattern matched, return a general command\n        return VoiceCommand(\n            action="unknown",\n            parameters={"text": transcript},\n            confidence=0.5\n        )\n'})}),"\n",(0,s.jsx)(n.h2,{id:"78-practical-exercise-voice-processing-system",children:"7.8 Practical Exercise: Voice Processing System"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-objective",children:"Exercise Objective"}),"\n",(0,s.jsx)(n.p,{children:"Create a complete voice processing system that integrates Whisper with ROS 2 for robotic command recognition."}),"\n",(0,s.jsx)(n.h3,{id:"steps",children:"Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up Whisper model with appropriate size for your hardware"}),"\n",(0,s.jsx)(n.li,{children:"Implement real-time audio capture and processing"}),"\n",(0,s.jsx)(n.li,{children:"Add voice activity detection to optimize processing"}),"\n",(0,s.jsx)(n.li,{children:"Implement command parsing for robotic actions"}),"\n",(0,s.jsx)(n.li,{children:"Test the system with various voice commands"}),"\n",(0,s.jsx)(n.li,{children:"Validate accuracy and response time"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"expected-results",children:"Expected Results"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Real-time voice processing with ",(0,s.jsx)(n.code,{children:"<2"})," second latency"]}),"\n",(0,s.jsx)(n.li,{children:"Accurate transcription (>80% accuracy in quiet conditions)"}),"\n",(0,s.jsx)(n.li,{children:"Proper command recognition and parsing"}),"\n",(0,s.jsx)(n.li,{children:"Integration with ROS 2 messaging system"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"Voice processing with Whisper provides a robust foundation for natural human-robot interaction. By properly integrating Whisper with ROS 2 and implementing voice activity detection, we can create responsive voice interfaces for humanoid robots. The key is balancing accuracy, latency, and computational efficiency for real-world robotic applications."}),"\n",(0,s.jsx)(n.h2,{id:"key-terms",children:"Key Terms"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Whisper"}),": OpenAI's automatic speech recognition system"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Voice Activity Detection (VAD)"}),": System to detect when speech is present"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time Processing"}),": Processing audio as it's captured"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Acoustic Conditions"}),": Environmental factors affecting audio quality"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Command Parsing"}),": Converting natural language to structured commands"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"OpenAI Whisper GitHub"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/snakers4/silero-vad",children:"Silero VAD"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://pyaudio.readthedocs.io/",children:"PyAudio Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2104.02744",children:"Speech Recognition in Robotics"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);