"use strict";(globalThis.webpackChunkai_robotics_textbook=globalThis.webpackChunkai_robotics_textbook||[]).push([[8985],{1376:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>r,contentTitle:()=>l,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4-vla/chapter-8","title":"Chapter 8: LLM-Based Cognitive Planning to ROS Actions","description":"Overview","source":"@site/docs/module-4-vla/chapter-8.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-8","permalink":"/ai-robotics-textbook/docs/module-4-vla/chapter-8","draft":false,"unlisted":false,"editUrl":"https://github.com/humonide-book/ai-robotics-textbook/edit/main/docs/module-4-vla/chapter-8.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_label":"Chapter 8: LLM-Based Cognitive Planning to ROS Actions","sidebar_position":2},"sidebar":"textbookSidebar","previous":{"title":"Chapter 7: Voice Processing with Whisper","permalink":"/ai-robotics-textbook/docs/module-4-vla/chapter-7"},"next":{"title":"Capstone Project: Autonomous Humanoid","permalink":"/ai-robotics-textbook/docs/module-4-vla/capstone-project"}}');var a=t(4848),s=t(8453);const o={sidebar_label:"Chapter 8: LLM-Based Cognitive Planning to ROS Actions",sidebar_position:2},l="Chapter 8: LLM-Based Cognitive Planning to ROS Actions",r={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"8.1 Introduction to LLM-Based Cognitive Planning",id:"81-introduction-to-llm-based-cognitive-planning",level:2},{value:"Cognitive Planning in Robotics",id:"cognitive-planning-in-robotics",level:3},{value:"Role of LLMs in Cognitive Planning",id:"role-of-llms-in-cognitive-planning",level:3},{value:"Architecture Overview",id:"architecture-overview",level:3},{value:"8.2 LLM Integration with ROS 2",id:"82-llm-integration-with-ros-2",level:2},{value:"Supported LLM Platforms",id:"supported-llm-platforms",level:3},{value:"Basic LLM Integration Example",id:"basic-llm-integration-example",level:3},{value:"8.3 Cognitive Architecture Design",id:"83-cognitive-architecture-design",level:2},{value:"Planning Hierarchy",id:"planning-hierarchy",level:3},{value:"Context Management",id:"context-management",level:3},{value:"Plan Validation and Safety",id:"plan-validation-and-safety",level:3},{value:"8.4 Multi-Step Task Planning",id:"84-multi-step-task-planning",level:2},{value:"Sequential Task Execution",id:"sequential-task-execution",level:3},{value:"8.5 LLM Prompt Engineering for Robotics",id:"85-llm-prompt-engineering-for-robotics",level:2},{value:"Effective Prompting Strategies",id:"effective-prompting-strategies",level:3},{value:"8.6 Handling Uncertainty and Adaptation",id:"86-handling-uncertainty-and-adaptation",level:2},{value:"Plan Adaptation System",id:"plan-adaptation-system",level:3},{value:"8.7 Integration with ROS 2 Ecosystem",id:"87-integration-with-ros-2-ecosystem",level:2},{value:"Service Definitions",id:"service-definitions",level:3},{value:"Action Integration",id:"action-integration",level:3},{value:"8.8 Practical Exercise: Cognitive Planning System",id:"88-practical-exercise-cognitive-planning-system",level:2},{value:"Exercise Objective",id:"exercise-objective",level:3},{value:"Steps",id:"steps",level:3},{value:"Expected Results",id:"expected-results",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Terms",id:"key-terms",level:2},{value:"References",id:"references",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"chapter-8-llm-based-cognitive-planning-to-ros-actions",children:"Chapter 8: LLM-Based Cognitive Planning to ROS Actions"})}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(e.p,{children:"This chapter covers the implementation of Large Language Model (LLM)-based cognitive planning systems that translate high-level natural language commands into executable ROS actions for humanoid robots. Students will learn to design cognitive architectures that enable robots to understand complex commands, plan multi-step tasks, and execute coordinated behaviors."}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"After completing this chapter, students will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Integrate LLMs with ROS 2 for cognitive planning"}),"\n",(0,a.jsx)(e.li,{children:"Design cognitive architectures for task planning and execution"}),"\n",(0,a.jsx)(e.li,{children:"Convert natural language commands to structured ROS action sequences"}),"\n",(0,a.jsx)(e.li,{children:"Implement multi-step task planning and execution"}),"\n",(0,a.jsx)(e.li,{children:"Handle task failures and plan adaptations"}),"\n",(0,a.jsx)(e.li,{children:"Validate cognitive planning accuracy and safety"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"81-introduction-to-llm-based-cognitive-planning",children:"8.1 Introduction to LLM-Based Cognitive Planning"}),"\n",(0,a.jsx)(e.h3,{id:"cognitive-planning-in-robotics",children:"Cognitive Planning in Robotics"}),"\n",(0,a.jsx)(e.p,{children:"Cognitive planning for robots involves:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Understanding"}),": Interpreting high-level commands in natural language"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Reasoning"}),": Determining appropriate sequences of actions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Planning"}),": Creating detailed execution plans with temporal and spatial constraints"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Execution"}),": Coordinating ROS nodes and services to execute the plan"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Monitoring"}),": Tracking plan execution and adapting to changes"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"role-of-llms-in-cognitive-planning",children:"Role of LLMs in Cognitive Planning"}),"\n",(0,a.jsx)(e.p,{children:"Large Language Models serve as:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Natural Language Interface"}),": Converting human commands to structured representations"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"World Model"}),": Maintaining knowledge about the environment and objects"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Reasoning Engine"}),": Determining appropriate action sequences"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Adaptation System"}),": Handling unexpected situations and plan modifications"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"Natural Language Command\n         \u2193\n    [LLM] - Cognitive Reasoning\n         \u2193\nStructured Action Plan\n         \u2193\n   [Planner] - Task Decomposition\n         \u2193\nROS Action Sequence\n         \u2193\n   [Executor] - ROS Node Coordination\n         \u2193\nPhysical Robot Actions\n"})}),"\n",(0,a.jsx)(e.h2,{id:"82-llm-integration-with-ros-2",children:"8.2 LLM Integration with ROS 2"}),"\n",(0,a.jsx)(e.h3,{id:"supported-llm-platforms",children:"Supported LLM Platforms"}),"\n",(0,a.jsx)(e.p,{children:"For robotics applications, several LLM platforms can be integrated:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"OpenAI GPT Models"}),": High accuracy, cloud-based, requires API access"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Anthropic Claude"}),": Strong reasoning capabilities, cloud-based"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Open-Source Models"}),": Mistral, Llama 2/3, locally deployable"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Specialized Robotics Models"}),": Models fine-tuned for robotics tasks"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"basic-llm-integration-example",children:"Basic LLM Integration Example"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nimport openai\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nfrom sensor_msgs.msg import Image\nimport json\nimport time\n\nclass LLMCognitivePlanner(Node):\n    def __init__(self):\n        super().__init__(\'llm_cognitive_planner\')\n\n        # Initialize LLM client (using OpenAI as example)\n        # In practice, you might use other LLM providers or local models\n        self.llm_client = None  # Initialize based on your chosen provider\n\n        # Subscribers for various inputs\n        self.voice_command_sub = self.create_subscription(\n            String,\n            \'voice_command\',\n            self.voice_command_callback,\n            10\n        )\n\n        self.text_command_sub = self.create_subscription(\n            String,\n            \'text_command\',\n            self.text_command_callback,\n            10\n        )\n\n        # Publishers for planning outputs\n        self.plan_pub = self.create_publisher(\n            String,\n            \'cognitive_plan\',\n            10\n        )\n\n        self.action_pub = self.create_publisher(\n            String,\n            \'planned_actions\',\n            10\n        )\n\n        # Service clients for robot capabilities\n        self.nav_client = self.create_client(\n            NavigateToPose,\n            \'navigate_to_pose\'\n        )\n\n        self.manipulation_client = self.create_client(\n            ManipulationCommand,\n            \'manipulation_command\'\n        )\n\n        self.get_logger().info(\'LLM Cognitive Planner initialized\')\n\n    def voice_command_callback(self, msg):\n        """Process voice command through LLM cognitive planner"""\n        self.process_command(msg.data, command_type=\'voice\')\n\n    def text_command_callback(self, msg):\n        """Process text command through LLM cognitive planner"""\n        self.process_command(msg.data, command_type=\'text\')\n\n    def process_command(self, command_text, command_type=\'voice\'):\n        """Process command using LLM cognitive planning"""\n        try:\n            # Generate cognitive plan using LLM\n            plan = self.generate_plan(command_text)\n\n            if plan:\n                # Publish the plan\n                plan_msg = String()\n                plan_msg.data = json.dumps(plan)\n                self.plan_pub.publish(plan_msg)\n\n                # Execute the plan\n                self.execute_plan(plan)\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing command: {e}\')\n\n    def generate_plan(self, command_text):\n        """Generate cognitive plan using LLM"""\n        # Construct prompt for the LLM\n        prompt = f"""\n        You are a cognitive planner for a humanoid robot. Your task is to interpret the following command and create a detailed execution plan.\n\n        Command: "{command_text}"\n\n        The robot has the following capabilities:\n        - Navigation: Can move to specified locations\n        - Manipulation: Can pick up and place objects\n        - Perception: Can detect objects in the environment\n        - Interaction: Can respond to humans and perform social behaviors\n\n        Please provide a structured plan in JSON format with the following schema:\n        {{\n            "command": "original command",\n            "intent": "high-level intent",\n            "steps": [\n                {{\n                    "action": "action_type",\n                    "parameters": {{}},\n                    "description": "what this step does"\n                }}\n            ],\n            "objects": ["list of objects mentioned"],\n            "locations": ["list of locations mentioned"],\n            "confidence": float (0-1)\n        }}\n\n        Be specific about locations, objects, and actions. If the command is unclear, ask for clarification.\n        """\n\n        try:\n            # Call the LLM (this is a simplified example)\n            # In practice, you\'d use the appropriate API call for your LLM\n            response = self.call_llm(prompt)\n            plan = json.loads(response)\n            return plan\n        except Exception as e:\n            self.get_logger().error(f\'Error generating plan: {e}\')\n            return None\n\n    def call_llm(self, prompt):\n        """Call the LLM with the given prompt"""\n        # This is a placeholder - implement based on your chosen LLM provider\n        # For example, with OpenAI:\n        # response = openai.ChatCompletion.create(\n        #     model="gpt-3.5-turbo",\n        #     messages=[{"role": "user", "content": prompt}],\n        #     temperature=0.1\n        # )\n        # return response.choices[0].message.content\n\n        # For now, return a mock response for demonstration\n        return \'\'\'\n        {\n            "command": "Go to the kitchen and bring me a cup",\n            "intent": "fetch object from specific location",\n            "steps": [\n                {\n                    "action": "navigate",\n                    "parameters": {"location": "kitchen"},\n                    "description": "Navigate to the kitchen area"\n                },\n                {\n                    "action": "detect",\n                    "parameters": {"object": "cup"},\n                    "description": "Look for a cup in the environment"\n                },\n                {\n                    "action": "manipulate",\n                    "parameters": {"action": "pick", "object": "cup"},\n                    "description": "Pick up the detected cup"\n                },\n                {\n                    "action": "navigate",\n                    "parameters": {"location": "starting_position"},\n                    "description": "Return to the starting position"\n                },\n                {\n                    "action": "manipulate",\n                    "parameters": {"action": "place", "object": "cup"},\n                    "description": "Place the cup down"\n                }\n            ],\n            "objects": ["cup"],\n            "locations": ["kitchen", "starting_position"],\n            "confidence": 0.9\n        }\n        \'\'\'\n'})}),"\n",(0,a.jsx)(e.h2,{id:"83-cognitive-architecture-design",children:"8.3 Cognitive Architecture Design"}),"\n",(0,a.jsx)(e.h3,{id:"planning-hierarchy",children:"Planning Hierarchy"}),"\n",(0,a.jsx)(e.p,{children:"Cognitive planning typically follows a hierarchical structure:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Task Level"}),": High-level goals and objectives"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Action Level"}),": Sequences of actions to achieve goals"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Motion Level"}),": Specific robot motions and configurations"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Execution Level"}),": Low-level control commands"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"context-management",children:"Context Management"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class ContextManager:\n    def __init__(self):\n        self.current_task = None\n        self.task_history = []\n        self.environment_state = {}\n        self.robot_state = {}\n        self.user_preferences = {}\n\n    def update_environment_state(self, sensor_data):\n        """Update environment state based on sensor data"""\n        # Process sensor data and update environment model\n        pass\n\n    def get_context_prompt(self):\n        """Generate context for LLM queries"""\n        context = {\n            "environment": self.environment_state,\n            "robot": self.robot_state,\n            "task_history": self.task_history[-5:],  # Last 5 tasks\n            "current_task": self.current_task\n        }\n        return json.dumps(context, indent=2)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"plan-validation-and-safety",children:"Plan Validation and Safety"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class PlanValidator:\n    def __init__(self):\n        self.safety_rules = [\n            self.check_collision_risk,\n            self.check_physical_limits,\n            self.check_human_safety\n        ]\n\n    def validate_plan(self, plan):\n        """Validate plan for safety and feasibility"""\n        for rule in self.safety_rules:\n            if not rule(plan):\n                return False, f"Plan failed {rule.__name__} check"\n        return True, "Plan is valid"\n\n    def check_collision_risk(self, plan):\n        """Check if plan involves potential collisions"""\n        # Implement collision checking logic\n        return True\n\n    def check_physical_limits(self, plan):\n        """Check if plan respects robot physical limits"""\n        # Implement physical limit checking\n        return True\n\n    def check_human_safety(self, plan):\n        """Check if plan is safe around humans"""\n        # Implement human safety checking\n        return True\n'})}),"\n",(0,a.jsx)(e.h2,{id:"84-multi-step-task-planning",children:"8.4 Multi-Step Task Planning"}),"\n",(0,a.jsx)(e.h3,{id:"sequential-task-execution",children:"Sequential Task Execution"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class TaskExecutor:\n    def __init__(self, node):\n        self.node = node\n        self.current_plan = None\n        self.current_step = 0\n        self.execution_status = "idle"\n\n    def execute_plan(self, plan):\n        """Execute a multi-step plan"""\n        self.current_plan = plan\n        self.current_step = 0\n        self.execution_status = "executing"\n\n        while self.current_step < len(plan[\'steps\']) and self.execution_status == "executing":\n            step = plan[\'steps\'][self.current_step]\n\n            success = self.execute_step(step)\n\n            if success:\n                self.current_step += 1\n            else:\n                self.handle_failure(step)\n                break\n\n        self.execution_status = "completed" if self.current_step >= len(plan[\'steps\']) else "failed"\n\n    def execute_step(self, step):\n        """Execute a single step in the plan"""\n        action_type = step[\'action\']\n        parameters = step[\'parameters\']\n\n        self.node.get_logger().info(f\'Executing step: {step["description"]}\')\n\n        if action_type == \'navigate\':\n            return self.execute_navigation(parameters)\n        elif action_type == \'detect\':\n            return self.execute_detection(parameters)\n        elif action_type == \'manipulate\':\n            return self.execute_manipulation(parameters)\n        elif action_type == \'interact\':\n            return self.execute_interaction(parameters)\n        else:\n            self.node.get_logger().error(f\'Unknown action type: {action_type}\')\n            return False\n\n    def execute_navigation(self, params):\n        """Execute navigation step"""\n        # Create navigation goal\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose = self.get_pose_for_location(params[\'location\'])\n\n        # Send navigation goal\n        future = self.node.nav_client.call_async(goal_msg)\n\n        # Wait for result (with timeout)\n        rclpy.spin_until_future_complete(self.node, future, timeout_sec=30.0)\n\n        if future.result() is not None:\n            return future.result().result.success\n        else:\n            return False\n\n    def execute_manipulation(self, params):\n        """Execute manipulation step"""\n        # Create manipulation goal\n        goal_msg = ManipulationCommand.Goal()\n        goal_msg.action = params[\'action\']\n        if \'object\' in params:\n            goal_msg.object_name = params[\'object\']\n\n        # Send manipulation goal\n        future = self.node.manipulation_client.call_async(goal_msg)\n\n        # Wait for result (with timeout)\n        rclpy.spin_until_future_complete(self.node, future, timeout_sec=10.0)\n\n        if future.result() is not None:\n            return future.result().result.success\n        else:\n            return False\n'})}),"\n",(0,a.jsx)(e.h2,{id:"85-llm-prompt-engineering-for-robotics",children:"8.5 LLM Prompt Engineering for Robotics"}),"\n",(0,a.jsx)(e.h3,{id:"effective-prompting-strategies",children:"Effective Prompting Strategies"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class PromptEngineer:\n    def __init__(self):\n        self.system_prompt = """\n        You are a cognitive planning assistant for a humanoid robot. Your role is to:\n        1. Interpret natural language commands from humans\n        2. Create detailed, executable plans for the robot\n        3. Consider safety, feasibility, and efficiency\n        4. Ask for clarification when commands are ambiguous\n\n        Always respond in valid JSON format with the specified schema.\n        """\n\n    def create_planning_prompt(self, command, context):\n        """Create a prompt for cognitive planning"""\n        return f"""\n        {self.system_prompt}\n\n        CONTEXT:\n        {context}\n\n        COMMAND: {command}\n\n        Please provide a detailed plan in the following JSON format:\n        {{\n            "command": "{command}",\n            "intent": "high-level intent",\n            "steps": [\n                {{\n                    "action": "action_type",\n                    "parameters": {{}},\n                    "description": "what this step does",\n                    "expected_outcome": "what should happen after this step"\n                }}\n            ],\n            "objects": ["list", "of", "relevant", "objects"],\n            "locations": ["list", "of", "relevant", "locations"],\n            "confidence": 0.0-1.0,\n            "reasoning": "brief explanation of your plan"\n        }}\n\n        Requirements:\n        - Each step should be executable by the robot\n        - Include error handling where appropriate\n        - Consider the current state of the world\n        - Prioritize safety in all actions\n        """\n\n    def create_clarification_prompt(self, ambiguous_command, context):\n        """Create a prompt to ask for clarification"""\n        return f"""\n        {self.system_prompt}\n\n        CONTEXT:\n        {context}\n\n        AMBIGUOUS COMMAND: {ambiguous_command}\n\n        The command is ambiguous. Please ask specific questions to clarify:\n        1. Which specific object is meant?\n        2. Which specific location is meant?\n        3. What exactly should the robot do?\n\n        Respond with a list of specific questions to ask the user.\n        """\n'})}),"\n",(0,a.jsx)(e.h2,{id:"86-handling-uncertainty-and-adaptation",children:"8.6 Handling Uncertainty and Adaptation"}),"\n",(0,a.jsx)(e.h3,{id:"plan-adaptation-system",children:"Plan Adaptation System"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class PlanAdaptationSystem:\n    def __init__(self):\n        self.adaptation_strategies = {\n            \'object_not_found\': self.handle_object_not_found,\n            \'location_not_reachable\': self.handle_location_not_reachable,\n            \'action_failed\': self.handle_action_failed\n        }\n\n    def handle_failure(self, failed_step, error_type):\n        """Handle plan execution failure"""\n        if error_type in self.adaptation_strategies:\n            return self.adaptation_strategies[error_type](failed_step)\n        else:\n            return self.generic_failure_handling(failed_step)\n\n    def handle_object_not_found(self, step):\n        """Handle case where expected object is not found"""\n        # Ask LLM for alternative approaches\n        prompt = f"""\n        The robot was looking for {step[\'parameters\'].get(\'object\', \'an object\')}\n        but could not find it. What should the robot do next?\n\n        Possible alternatives:\n        1. Look in other locations\n        2. Ask user for help\n        3. Use a substitute object\n        4. Abort the task\n\n        Respond with a new plan step or a list of options.\n        """\n\n        # Call LLM for adaptation strategy\n        # Return new plan or continue with adaptation\n        pass\n\n    def handle_location_not_reachable(self, step):\n        """Handle case where navigation target is not reachable"""\n        # Implement navigation adaptation\n        pass\n\n    def handle_action_failed(self, step):\n        """Handle case where an action failed"""\n        # Implement action-specific adaptation\n        pass\n'})}),"\n",(0,a.jsx)(e.h2,{id:"87-integration-with-ros-2-ecosystem",children:"8.7 Integration with ROS 2 Ecosystem"}),"\n",(0,a.jsx)(e.h3,{id:"service-definitions",children:"Service Definitions"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# cognitive_planning_interfaces/srv/GeneratePlan.srv\nstring command\nstring context\n---\nstring plan_json\nfloat64 confidence\nbool success\nstring error_message\n\n# cognitive_planning_interfaces/srv/ExecutePlan.srv\nstring plan_json\n---\nbool success\nstring status\nfloat64 execution_time\n"})}),"\n",(0,a.jsx)(e.h3,{id:"action-integration",children:"Action Integration"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from rclpy.action import ActionClient\nfrom cognitive_planning_interfaces.action import ExecuteCognitivePlan\n\nclass LLMPlannerWithActions(LLMCognitivePlanner):\n    def __init__(self):\n        super().__init__()\n\n        # Create action client for plan execution\n        self.plan_execution_client = ActionClient(\n            self,\n            ExecuteCognitivePlan,\n            \'execute_cognitive_plan\'\n        )\n\n    def execute_plan_with_feedback(self, plan):\n        """Execute plan with progress feedback"""\n        goal_msg = ExecuteCognitivePlan.Goal()\n        goal_msg.plan_json = json.dumps(plan)\n        goal_msg.timeout = rclpy.Duration(seconds=300)  # 5 minute timeout\n\n        # Wait for action server\n        self.plan_execution_client.wait_for_server()\n\n        # Send goal and get feedback\n        future = self.plan_execution_client.send_goal_async(\n            goal_msg,\n            feedback_callback=self.plan_feedback_callback\n        )\n\n        future.add_done_callback(self.plan_complete_callback)\n\n    def plan_feedback_callback(self, feedback_msg):\n        """Handle plan execution feedback"""\n        self.get_logger().info(f\'Plan progress: {feedback_msg.feedback.progress}%\')\n\n    def plan_complete_callback(self, future):\n        """Handle plan completion"""\n        goal_handle = future.result()\n        result = goal_handle.get_result_async()\n        result.add_done_callback(self.plan_result_callback)\n\n    def plan_result_callback(self, future):\n        """Handle plan execution result"""\n        result_msg = future.result().result\n        self.get_logger().info(f\'Plan execution result: {result_msg.success}\')\n'})}),"\n",(0,a.jsx)(e.h2,{id:"88-practical-exercise-cognitive-planning-system",children:"8.8 Practical Exercise: Cognitive Planning System"}),"\n",(0,a.jsx)(e.h3,{id:"exercise-objective",children:"Exercise Objective"}),"\n",(0,a.jsx)(e.p,{children:"Create a complete cognitive planning system that takes natural language commands and executes them on a simulated humanoid robot."}),"\n",(0,a.jsx)(e.h3,{id:"steps",children:"Steps"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Set up LLM integration with ROS 2"}),"\n",(0,a.jsx)(e.li,{children:"Implement context management system"}),"\n",(0,a.jsx)(e.li,{children:"Create plan validation and safety checks"}),"\n",(0,a.jsx)(e.li,{children:"Develop multi-step task execution"}),"\n",(0,a.jsx)(e.li,{children:"Add failure handling and adaptation"}),"\n",(0,a.jsx)(e.li,{children:"Test with various natural language commands"}),"\n",(0,a.jsx)(e.li,{children:"Validate safety and correctness"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"expected-results",children:"Expected Results"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Natural language command understanding (>80% accuracy)"}),"\n",(0,a.jsx)(e.li,{children:"Safe and correct plan execution"}),"\n",(0,a.jsx)(e.li,{children:"Proper failure handling and adaptation"}),"\n",(0,a.jsx)(e.li,{children:"Integration with ROS 2 messaging and services"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"LLM-based cognitive planning provides a powerful approach to bridge natural language commands with robotic action execution. By properly designing the cognitive architecture, implementing safety checks, and handling uncertainty, we can create intelligent humanoid robots capable of understanding and executing complex tasks through natural interaction."}),"\n",(0,a.jsx)(e.h2,{id:"key-terms",children:"Key Terms"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Cognitive Planning"}),": High-level planning using reasoning and knowledge"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Large Language Model (LLM)"}),": AI model for natural language understanding"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Task Decomposition"}),": Breaking complex tasks into simpler steps"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Plan Validation"}),": Checking plans for safety and feasibility"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Adaptation System"}),": Handling plan failures and unexpected situations"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"references",children:"References"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://platform.openai.com/docs/",children:"OpenAI API Documentation"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://arxiv.org/abs/2303.04650",children:"Robotics and AI Integration"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://arxiv.org/abs/2205.12258",children:"Natural Language to Robot Action"})}),"\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://arxiv.org/abs/2103.06759",children:"Cognitive Architectures for Robotics"})}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>l});var i=t(6540);const a={},s=i.createContext(a);function o(n){const e=i.useContext(s);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:o(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);