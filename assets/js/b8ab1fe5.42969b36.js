"use strict";(globalThis.webpackChunkai_robotics_textbook=globalThis.webpackChunkai_robotics_textbook||[]).push([[4211],{1930:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>_,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module-3-ai-robot-brain/chapter-6","title":"Chapter 6: Isaac ROS (VSLAM, Navigation, Perception)","description":"Overview","source":"@site/docs/module-3-ai-robot-brain/chapter-6.md","sourceDirName":"module-3-ai-robot-brain","slug":"/module-3-ai-robot-brain/chapter-6","permalink":"/ai-robotics-textbook/docs/module-3-ai-robot-brain/chapter-6","draft":false,"unlisted":false,"editUrl":"https://github.com/humonide-book/ai-robotics-textbook/edit/main/docs/module-3-ai-robot-brain/chapter-6.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_label":"Chapter 6: Isaac ROS (VSLAM, Navigation, Perception)","sidebar_position":2},"sidebar":"textbookSidebar","previous":{"title":"Chapter 5: Isaac Sim and Synthetic Data Generation","permalink":"/ai-robotics-textbook/docs/module-3-ai-robot-brain/chapter-5"},"next":{"title":"Mini-Project: Perception + Navigation Stack","permalink":"/ai-robotics-textbook/docs/module-3-ai-robot-brain/mini-project"}}');var a=i(4848),t=i(8453);const o={sidebar_label:"Chapter 6: Isaac ROS (VSLAM, Navigation, Perception)",sidebar_position:2},s="Chapter 6: Isaac ROS (VSLAM, Navigation, Perception)",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"6.1 Introduction to Isaac ROS",id:"61-introduction-to-isaac-ros",level:2},{value:"What is Isaac ROS?",id:"what-is-isaac-ros",level:3},{value:"Key Isaac ROS Packages",id:"key-isaac-ros-packages",level:3},{value:"Hardware Requirements",id:"hardware-requirements",level:3},{value:"6.2 Isaac ROS Visual SLAM",id:"62-isaac-ros-visual-slam",level:2},{value:"Visual SLAM Fundamentals",id:"visual-slam-fundamentals",level:3},{value:"Isaac ROS Visual SLAM Architecture",id:"isaac-ros-visual-slam-architecture",level:3},{value:"Installation and Setup",id:"installation-and-setup",level:3},{value:"Example Configuration",id:"example-configuration",level:3},{value:"Launch Configuration",id:"launch-configuration",level:3},{value:"6.3 Isaac ROS Navigation Stack",id:"63-isaac-ros-navigation-stack",level:2},{value:"Navigation 2 Integration",id:"navigation-2-integration",level:3},{value:"Navigation Configuration",id:"navigation-configuration",level:3},{value:"6.4 Isaac ROS Perception Systems",id:"64-isaac-ros-perception-systems",level:2},{value:"Stereo DNN Pipeline",id:"stereo-dnn-pipeline",level:3},{value:"Isaac ROS Apriltag Detection",id:"isaac-ros-apriltag-detection",level:3},{value:"6.5 Isaac ROS Integration with Humanoid Robots",id:"65-isaac-ros-integration-with-humanoid-robots",level:2},{value:"Sensor Integration",id:"sensor-integration",level:3},{value:"Example Integration Node",id:"example-integration-node",level:3},{value:"6.6 Performance Optimization",id:"66-performance-optimization",level:2},{value:"GPU Utilization",id:"gpu-utilization",level:3},{value:"Example GPU Memory Management",id:"example-gpu-memory-management",level:3},{value:"6.7 Practical Exercise: VSLAM Implementation",id:"67-practical-exercise-vslam-implementation",level:2},{value:"Exercise Objective",id:"exercise-objective",level:3},{value:"Steps",id:"steps",level:3},{value:"Expected Results",id:"expected-results",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Terms",id:"key-terms",level:2},{value:"References",id:"references",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"chapter-6-isaac-ros-vslam-navigation-perception",children:"Chapter 6: Isaac ROS (VSLAM, Navigation, Perception)"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"This chapter covers NVIDIA Isaac ROS, a collection of GPU-accelerated perception and navigation packages for robotics. Students will learn to implement Visual Simultaneous Localization and Mapping (VSLAM), navigation systems, and perception algorithms using Isaac ROS packages. These tools enable real-time processing of sensor data for autonomous robot operation."}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"After completing this chapter, students will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Install and configure NVIDIA Isaac ROS packages"}),"\n",(0,a.jsx)(n.li,{children:"Implement VSLAM systems for robot localization and mapping"}),"\n",(0,a.jsx)(n.li,{children:"Design navigation pipelines for autonomous movement"}),"\n",(0,a.jsx)(n.li,{children:"Create perception systems for environment understanding"}),"\n",(0,a.jsx)(n.li,{children:"Integrate Isaac ROS with existing ROS 2 systems"}),"\n",(0,a.jsx)(n.li,{children:"Optimize perception algorithms for real-time performance"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"61-introduction-to-isaac-ros",children:"6.1 Introduction to Isaac ROS"}),"\n",(0,a.jsx)(n.h3,{id:"what-is-isaac-ros",children:"What is Isaac ROS?"}),"\n",(0,a.jsx)(n.p,{children:"Isaac ROS is a collection of GPU-accelerated packages that bring NVIDIA's AI and computer vision capabilities to ROS 2. It includes optimized implementations of common robotics algorithms that leverage GPU parallelism for real-time performance."}),"\n",(0,a.jsx)(n.h3,{id:"key-isaac-ros-packages",children:"Key Isaac ROS Packages"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS Visual SLAM"}),": GPU-accelerated visual SLAM"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS Apriltag"}),": High-performance AprilTag detection"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS Stereo DNN"}),": Stereo vision with deep learning"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS NITROS"}),": Network Interface for Trust, Reliability, and Safety"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS DLA"}),": Deep Learning Accelerators integration"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"NVIDIA Jetson AGX Orin, Jetson Orin NX, or discrete GPU"}),"\n",(0,a.jsx)(n.li,{children:"CUDA-compatible GPU (Compute Capability 6.0+)"}),"\n",(0,a.jsx)(n.li,{children:"Sufficient memory for GPU operations"}),"\n",(0,a.jsx)(n.li,{children:"Compatible camera and sensor systems"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"62-isaac-ros-visual-slam",children:"6.2 Isaac ROS Visual SLAM"}),"\n",(0,a.jsx)(n.h3,{id:"visual-slam-fundamentals",children:"Visual SLAM Fundamentals"}),"\n",(0,a.jsx)(n.p,{children:"Visual SLAM (Simultaneous Localization and Mapping) allows robots to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Build a map of unknown environments"}),"\n",(0,a.jsx)(n.li,{children:"Simultaneously determine their position within the map"}),"\n",(0,a.jsx)(n.li,{children:"Track their movement over time"}),"\n",(0,a.jsx)(n.li,{children:"Enable autonomous navigation"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"isaac-ros-visual-slam-architecture",children:"Isaac ROS Visual SLAM Architecture"}),"\n",(0,a.jsx)(n.p,{children:"The Isaac ROS Visual SLAM pipeline includes:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Feature Detection"}),": Extract distinctive visual features"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Feature Matching"}),": Match features across frames"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Pose Estimation"}),": Estimate camera/robot motion"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Mapping"}),": Build 3D map of environment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Loop Closure"}),": Detect revisited locations"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Optimization"}),": Refine map and trajectory"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"installation-and-setup",children:"Installation and Setup"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Add NVIDIA ROS 2 repository\r\nsudo apt update\r\nsudo apt install software-properties-common\r\nsudo add-apt-repository universe\r\nsudo apt update\r\n\r\n# Install Isaac ROS Visual SLAM\r\nsudo apt install ros-humble-isaac-ros-visual-slam\n"})}),"\n",(0,a.jsx)(n.h3,{id:"example-configuration",children:"Example Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# visual_slam.yaml\r\nvisual_slam_node:\r\n  ros__parameters:\r\n    # Input topics\r\n    input_topic_camera_optical_frame: "camera_optical_frame"\r\n    input_topic_imu: "imu/data"\r\n\r\n    # Output topics\r\n    output_tracking_topic: "/visual_slam/tracking"\r\n    output_map_topic: "/visual_slam/map"\r\n    output_odom_topic: "/visual_slam/odometry"\r\n\r\n    # Parameters\r\n    enable_debug_mode: false\r\n    enable_imu_fusion: true\r\n    use_sim_time: false\r\n    publish_tf: true\r\n\r\n    # Tracking parameters\r\n    min_num_points: 100\r\n    max_num_points: 1000\r\n    min_distance_between_poses: 0.1\n'})}),"\n",(0,a.jsx)(n.h3,{id:"launch-configuration",children:"Launch Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:"\x3c!-- visual_slam.launch.py --\x3e\r\nfrom launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\nfrom ament_index_python.packages import get_package_share_directory\r\nimport os\r\n\r\ndef generate_launch_description():\r\n    config = os.path.join(\r\n        get_package_share_directory('my_robot_bringup'),\r\n        'config',\r\n        'visual_slam.yaml'\r\n    )\r\n\r\n    visual_slam_node = Node(\r\n        package='isaac_ros_visual_slam',\r\n        executable='visual_slam_node',\r\n        parameters=[config],\r\n        remappings=[\r\n            ('/visual_slam/camera/imu', '/camera/imu'),\r\n            ('/visual_slam/camera/rgb', '/camera/rgb'),\r\n            ('/visual_slam/imu', '/imu/data')\r\n        ]\r\n    )\r\n\r\n    return LaunchDescription([visual_slam_node])\n"})}),"\n",(0,a.jsx)(n.h2,{id:"63-isaac-ros-navigation-stack",children:"6.3 Isaac ROS Navigation Stack"}),"\n",(0,a.jsx)(n.h3,{id:"navigation-2-integration",children:"Navigation 2 Integration"}),"\n",(0,a.jsx)(n.p,{children:"Isaac ROS integrates with Navigation 2 (Nav2) to provide:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Global and local path planning"}),"\n",(0,a.jsx)(n.li,{children:"Obstacle avoidance"}),"\n",(0,a.jsx)(n.li,{children:"Dynamic obstacle detection"}),"\n",(0,a.jsx)(n.li,{children:"Behavior trees for complex navigation"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"navigation-configuration",children:"Navigation Configuration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# navigation_params.yaml\r\namcl:\r\n  ros__parameters:\r\n    use_sim_time: False\r\n    alpha1: 0.2\r\n    alpha2: 0.2\r\n    alpha3: 0.2\r\n    alpha4: 0.2\r\n    alpha5: 0.2\r\n    base_frame_id: "base_link"\r\n    beam_skip_distance: 0.5\r\n    beam_skip_error_threshold: 0.9\r\n    beam_skip_threshold: 0.3\r\n    do_beamskip: false\r\n    global_frame_id: "map"\r\n    lambda_short: 0.1\r\n    laser_likelihood_max_dist: 2.0\r\n    laser_max_range: 10.0\r\n    laser_min_range: -1.0\r\n    laser_model_type: "likelihood_field"\r\n    max_beams: 60\r\n    max_particles: 2000\r\n    min_particles: 500\r\n    odom_frame_id: "odom"\r\n    pf_err: 0.05\r\n    pf_z: 0.99\r\n    recovery_alpha_fast: 0.0\r\n    recovery_alpha_slow: 0.0\r\n    resample_interval: 1\r\n    robot_model_type: "nav2_amcl::DifferentialMotionModel"\r\n    save_pose_rate: 0.5\r\n    sigma_hit: 0.2\r\n    tf_broadcast: true\r\n    transform_tolerance: 1.0\r\n    update_min_a: 0.2\r\n    update_min_d: 0.25\r\n    z_hit: 0.5\r\n    z_max: 0.05\r\n    z_rand: 0.5\r\n    z_short: 0.05\r\n\r\nbt_navigator:\r\n  ros__parameters:\r\n    use_sim_time: false\r\n    global_frame: "map"\r\n    robot_base_frame: "base_link"\r\n    odom_topic: "/odom"\r\n    bt_loop_duration: 10\r\n    default_server_timeout: 20\r\n    enable_groot_monitoring: True\r\n    groot_zmq_publisher_port: 1666\r\n    groot_zmq_server_port: 1667\r\n    # Specify the path to the Behavior Tree XML file\r\n    default_nav_through_poses_bt_xml: "nav2_bt_xml_v0.14/navigate_through_poses_w_replanning_and_recovery.xml"\r\n    default_nav_to_pose_bt_xml: "nav2_bt_xml_v0.14/navigate_to_pose_w_replanning_and_recovery.xml"\r\n    plugin_lib_names:\r\n    - nav2_compute_path_to_pose_action_bt_node\r\n    - nav2_compute_path_through_poses_action_bt_node\r\n    - nav2_smooth_path_action_bt_node\r\n    - nav2_follow_path_action_bt_node\r\n    - nav2_spin_action_bt_node\r\n    - nav2_wait_action_bt_node\r\n    - nav2_assisted_teleop_action_bt_node\r\n    - nav2_back_up_action_bt_node\r\n    - nav2_drive_on_heading_bt_node\r\n    - nav2_clear_costmap_service_bt_node\r\n    - nav2_is_stuck_condition_bt_node\r\n    - nav2_goal_reached_condition_bt_node\r\n    - nav2_goal_updated_condition_bt_node\r\n    - nav2_globally_consistent_localization_condition_bt_node\r\n    - nav2_is_path_valid_condition_bt_node\r\n    - nav2_initial_pose_received_condition_bt_node\r\n    - nav2_reinitialize_global_localization_service_bt_node\r\n    - nav2_rate_controller_bt_node\r\n    - nav2_distance_controller_bt_node\r\n    - nav2_speed_controller_bt_node\r\n    - nav2_truncate_path_action_bt_node\r\n    - nav2_truncate_path_local_action_bt_node\r\n    - nav2_goal_updater_node_bt_node\r\n    - nav2_recovery_node_bt_node\r\n    - nav2_pipeline_sequence_bt_node\r\n    - nav2_round_robin_node_bt_node\r\n    - nav2_transform_available_condition_bt_node\r\n    - nav2_time_expired_condition_bt_node\r\n    - nav2_path_expiring_timer_condition\r\n    - nav2_distance_traveled_condition_bt_node\r\n    - nav2_single_trigger_bt_node\r\n    - nav2_is_battery_low_condition_bt_node\r\n    - nav2_navigate_through_poses_action_bt_node\r\n    - nav2_navigate_to_pose_action_bt_node\r\n    - nav2_remove_passed_goals_action_bt_node\r\n    - nav2_planner_selector_bt_node\r\n    - nav2_controller_selector_bt_node\r\n    - nav2_goal_checker_selector_bt_node\r\n    - nav2_controller_cancel_bt_node\r\n    - nav2_path_longer_on_approach_bt_node\r\n    - nav2_wait_cancel_bt_node\r\n    - nav2_spin_cancel_bt_node\r\n    - nav2_back_up_cancel_bt_node\r\n    - nav2_assisted_teleop_cancel_bt_node\r\n    - nav2_drive_on_heading_cancel_bt_node\r\n    - nav2_is_localizer_active_condition_bt_node\r\n    - nav2_is_map_loaded_condition_bt_node\r\n    - nav2_is_battery_charging_condition_bt_node\n'})}),"\n",(0,a.jsx)(n.h2,{id:"64-isaac-ros-perception-systems",children:"6.4 Isaac ROS Perception Systems"}),"\n",(0,a.jsx)(n.h3,{id:"stereo-dnn-pipeline",children:"Stereo DNN Pipeline"}),"\n",(0,a.jsx)(n.p,{children:"Isaac ROS provides GPU-accelerated stereo vision with deep learning:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom stereo_msgs.msg import DisparityImage\r\nfrom vision_msgs.msg import Detection2DArray\r\n\r\nclass StereoDNNNode(Node):\r\n    def __init__(self):\r\n        super().__init__('stereo_dnn_node')\r\n\r\n        # Subscriptions for stereo images\r\n        self.left_subscription = self.create_subscription(\r\n            Image,\r\n            'camera/left/image_rect_color',\r\n            self.left_image_callback,\r\n            10\r\n        )\r\n        self.right_subscription = self.create_subscription(\r\n            Image,\r\n            'camera/right/image_rect_color',\r\n            self.right_image_callback,\r\n            10\r\n        )\r\n\r\n        # Publisher for detections\r\n        self.detection_publisher = self.create_publisher(\r\n            Detection2DArray,\r\n            'detections',\r\n            10\r\n        )\r\n\r\n        self.get_logger().info('Stereo DNN node initialized')\r\n\r\n    def left_image_callback(self, msg):\r\n        # Process left image with Isaac ROS stereo DNN\r\n        self.get_logger().info(f'Received left image: {msg.width}x{msg.height}')\r\n\r\n    def right_image_callback(self, msg):\r\n        # Process right image with Isaac ROS stereo DNN\r\n        self.get_logger().info(f'Received right image: {msg.width}x{msg.height}')\n"})}),"\n",(0,a.jsx)(n.h3,{id:"isaac-ros-apriltag-detection",children:"Isaac ROS Apriltag Detection"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom vision_msgs.msg import Detection2DArray\r\nfrom geometry_msgs.msg import Point\r\n\r\nclass ApriltagDetectionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('apriltag_detection_node')\r\n\r\n        # Subscription to camera image\r\n        self.subscription = self.create_subscription(\r\n            Image,\r\n            'camera/image_rect_color',\r\n            self.image_callback,\r\n            10\r\n        )\r\n\r\n        # Publisher for tag detections\r\n        self.detection_publisher = self.create_publisher(\r\n            Detection2DArray,\r\n            'apriltag_detections',\r\n            10\r\n        )\r\n\r\n        self.get_logger().info('AprilTag detection node initialized')\r\n\r\n    def image_callback(self, msg):\r\n        # Process image with Isaac ROS Apriltag\r\n        # This would typically interface with the Isaac ROS Apriltag node\r\n        self.get_logger().info(f'Processing image for AprilTag detection: {msg.width}x{msg.height}')\n"})}),"\n",(0,a.jsx)(n.h2,{id:"65-isaac-ros-integration-with-humanoid-robots",children:"6.5 Isaac ROS Integration with Humanoid Robots"}),"\n",(0,a.jsx)(n.h3,{id:"sensor-integration",children:"Sensor Integration"}),"\n",(0,a.jsx)(n.p,{children:"For humanoid robots, Isaac ROS integration involves:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Multiple camera systems for 360\xb0 perception"}),"\n",(0,a.jsx)(n.li,{children:"IMU integration for pose estimation"}),"\n",(0,a.jsx)(n.li,{children:"LiDAR for environment mapping"}),"\n",(0,a.jsx)(n.li,{children:"Force/torque sensors for contact detection"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"example-integration-node",children:"Example Integration Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, Imu, PointCloud2\r\nfrom geometry_msgs.msg import Twist\r\nfrom nav_msgs.msg import Odometry\r\nimport numpy as np\r\n\r\nclass HumanoidPerceptionNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'humanoid_perception_node\')\r\n\r\n        # Subscriptions for all sensors\r\n        self.camera_subscription = self.create_subscription(\r\n            Image,\r\n            \'/head_camera/rgb/image_raw\',\r\n            self.camera_callback,\r\n            10\r\n        )\r\n\r\n        self.imu_subscription = self.create_subscription(\r\n            Imu,\r\n            \'/imu/data\',\r\n            self.imu_callback,\r\n            10\r\n        )\r\n\r\n        self.lidar_subscription = self.create_subscription(\r\n            PointCloud2,\r\n            \'/scan\',\r\n            self.lidar_callback,\r\n            10\r\n        )\r\n\r\n        # Publisher for navigation commands\r\n        self.cmd_vel_publisher = self.create_publisher(\r\n            Twist,\r\n            \'/cmd_vel\',\r\n            10\r\n        )\r\n\r\n        # Publisher for processed perception data\r\n        self.perception_publisher = self.create_publisher(\r\n            Odometry,  # Or custom message type\r\n            \'/perception/processed_data\',\r\n            10\r\n        )\r\n\r\n        self.get_logger().info(\'Humanoid perception node initialized\')\r\n\r\n        # Initialize perception components\r\n        self.initialize_perception_pipeline()\r\n\r\n    def initialize_perception_pipeline(self):\r\n        """Initialize all perception components"""\r\n        self.get_logger().info(\'Initializing perception pipeline...\')\r\n        # Initialize VSLAM, object detection, etc.\r\n\r\n    def camera_callback(self, msg):\r\n        """Process camera data using Isaac ROS"""\r\n        # This would interface with Isaac ROS visual processing nodes\r\n        self.get_logger().info(f\'Processing camera data: {msg.width}x{msg.height}\')\r\n\r\n    def imu_callback(self, msg):\r\n        """Process IMU data for orientation"""\r\n        orientation = msg.orientation\r\n        angular_velocity = msg.angular_velocity\r\n        linear_acceleration = msg.linear_acceleration\r\n\r\n        # Process IMU data for navigation\r\n        self.get_logger().info(\'Processing IMU data\')\r\n\r\n    def lidar_callback(self, msg):\r\n        """Process LiDAR data for mapping and obstacle detection"""\r\n        # Process LiDAR data using Isaac ROS or custom algorithms\r\n        self.get_logger().info(\'Processing LiDAR data\')\n'})}),"\n",(0,a.jsx)(n.h2,{id:"66-performance-optimization",children:"6.6 Performance Optimization"}),"\n",(0,a.jsx)(n.h3,{id:"gpu-utilization",children:"GPU Utilization"}),"\n",(0,a.jsx)(n.p,{children:"To maximize Isaac ROS performance:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Use appropriate GPU memory allocation"}),"\n",(0,a.jsx)(n.li,{children:"Optimize batch sizes for inference"}),"\n",(0,a.jsx)(n.li,{children:"Utilize TensorRT for model optimization"}),"\n",(0,a.jsx)(n.li,{children:"Profile and optimize bottlenecks"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"example-gpu-memory-management",children:"Example GPU Memory Management"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import torch\r\nimport rclpy\r\n\r\ndef optimize_gpu_memory():\r\n    """Configure GPU memory for Isaac ROS operations"""\r\n    if torch.cuda.is_available():\r\n        # Set memory fraction to prevent out-of-memory errors\r\n        torch.cuda.set_per_process_memory_fraction(0.8)\r\n\r\n        # Clear GPU cache periodically\r\n        torch.cuda.empty_cache()\r\n\r\n        print(f"GPU memory allocated: {torch.cuda.memory_allocated()}")\r\n        print(f"GPU memory reserved: {torch.cuda.memory_reserved()}")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"67-practical-exercise-vslam-implementation",children:"6.7 Practical Exercise: VSLAM Implementation"}),"\n",(0,a.jsx)(n.h3,{id:"exercise-objective",children:"Exercise Objective"}),"\n",(0,a.jsx)(n.p,{children:"Implement a complete VSLAM system for a humanoid robot using Isaac ROS."}),"\n",(0,a.jsx)(n.h3,{id:"steps",children:"Steps"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Set up Isaac ROS Visual SLAM with camera and IMU"}),"\n",(0,a.jsx)(n.li,{children:"Configure mapping and localization parameters"}),"\n",(0,a.jsx)(n.li,{children:"Test VSLAM in simulation environment"}),"\n",(0,a.jsx)(n.li,{children:"Validate map quality and localization accuracy"}),"\n",(0,a.jsx)(n.li,{children:"Integrate with navigation stack"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"expected-results",children:"Expected Results"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Real-time VSLAM processing"}),"\n",(0,a.jsx)(n.li,{children:"Accurate localization and mapping"}),"\n",(0,a.jsx)(n.li,{children:"Integration with navigation system"}),"\n",(0,a.jsx)(n.li,{children:"Validated performance metrics"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"Isaac ROS provides powerful GPU-accelerated tools for robotics perception and navigation. By leveraging these packages, humanoid robots can achieve real-time processing of sensor data for localization, mapping, and autonomous navigation. The integration of Isaac ROS with traditional ROS 2 systems enables advanced AI capabilities for complex robotic tasks."}),"\n",(0,a.jsx)(n.h2,{id:"key-terms",children:"Key Terms"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"VSLAM"}),": Visual Simultaneous Localization and Mapping"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS"}),": NVIDIA's GPU-accelerated ROS packages"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Navigation 2"}),": ROS 2 navigation stack (Nav2)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"TensorRT"}),": NVIDIA's inference optimizer"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"NITROS"}),": Network Interface for Trust, Reliability, and Safety"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://nvidia-isaac-ros.github.io/repositories_and_packages/index.html",children:"Isaac ROS Documentation"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://navigation.ros.org/",children:"Navigation 2 Documentation"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://arxiv.org/abs/1606.05830",children:"Visual SLAM Tutorial"})}),"\n"]})]})}function _(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>s});var r=i(6540);const a={},t=r.createContext(a);function o(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);