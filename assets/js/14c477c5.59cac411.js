"use strict";(globalThis.webpackChunkai_robotics_textbook=globalThis.webpackChunkai_robotics_textbook||[]).push([[3431],{8453:(e,r,n)=>{n.d(r,{R:()=>o,x:()=>a});var i=n(6540);const s={},t=i.createContext(s);function o(e){const r=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function a(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(t.Provider,{value:r},e.children)}},8938:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-4-vla/chapter-7","title":"Chapter 7: Voice Processing with Whisper","description":"Overview","source":"@site/docs/module-4-vla/chapter-7.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/chapter-7","permalink":"/ai-robotics-textbook/docs/module-4-vla/chapter-7","draft":false,"unlisted":false,"editUrl":"https://github.com/humonide-book/ai-robotics-textbook/edit/main/docs/module-4-vla/chapter-7.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_label":"Chapter 7: Voice Processing with Whisper","sidebar_position":1},"sidebar":"textbookSidebar","previous":{"title":"Learning Objectives","permalink":"/ai-robotics-textbook/docs/module-4-vla/learning-objectives"},"next":{"title":"Chapter 8: LLM-Based Cognitive Planning to ROS Actions","permalink":"/ai-robotics-textbook/docs/module-4-vla/chapter-8"}}');var s=n(4848),t=n(8453);const o={sidebar_label:"Chapter 7: Voice Processing with Whisper",sidebar_position:1},a="Chapter 7: Voice Processing with Whisper",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"7.1 Introduction to Whisper for Robotics",id:"71-introduction-to-whisper-for-robotics",level:2},{value:"What is Whisper?",id:"what-is-whisper",level:3},{value:"Key Features for Robotics",id:"key-features-for-robotics",level:3},{value:"Whisper Model Variants",id:"whisper-model-variants",level:3},{value:"7.2 Installing and Setting Up Whisper",id:"72-installing-and-setting-up-whisper",level:2},{value:"System Requirements",id:"system-requirements",level:3},{value:"Installation Process",id:"installation-process",level:3},{value:"Basic Whisper Usage Example",id:"basic-whisper-usage-example",level:3},{value:"7.3 Whisper Integration with ROS 2",id:"73-whisper-integration-with-ros-2",level:2},{value:"Audio Input from Microphone",id:"audio-input-from-microphone",level:3},{value:"7.4 Voice Activity Detection",id:"74-voice-activity-detection",level:2},{value:"Importance in Robotics",id:"importance-in-robotics",level:3},{value:"Implementation with Silero VAD",id:"implementation-with-silero-vad",level:3},{value:"Integration with Whisper Processing",id:"integration-with-whisper-processing",level:3},{value:"7.5 Real-time Processing Optimization",id:"75-real-time-processing-optimization",level:2},{value:"Batch Processing",id:"batch-processing",level:3},{value:"7.6 Handling Acoustic Conditions",id:"76-handling-acoustic-conditions",level:2},{value:"Noise Reduction",id:"noise-reduction",level:3},{value:"7.7 Voice Command Parsing",id:"77-voice-command-parsing",level:2},{value:"Simple Command Recognition",id:"simple-command-recognition",level:3},{value:"7.8 Practical Exercise: Voice Processing System",id:"78-practical-exercise-voice-processing-system",level:2},{value:"Exercise Objective",id:"exercise-objective",level:3},{value:"Steps",id:"steps",level:3},{value:"Expected Results",id:"expected-results",level:3},{value:"Summary",id:"summary",level:2},{value:"Key Terms",id:"key-terms",level:2},{value:"References",id:"references",level:2}];function d(e){const r={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.header,{children:(0,s.jsx)(r.h1,{id:"chapter-7-voice-processing-with-whisper",children:"Chapter 7: Voice Processing with Whisper"})}),"\n",(0,s.jsx)(r.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(r.p,{children:"This chapter covers the implementation of voice processing systems using OpenAI's Whisper for speech-to-text conversion in humanoid robotics applications. Students will learn to integrate Whisper with ROS 2, process voice commands in real-time, and handle various acoustic conditions that may be encountered in real-world robotic environments."}),"\n",(0,s.jsx)(r.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(r.p,{children:"After completing this chapter, students will be able to:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsx)(r.li,{children:"Install and configure OpenAI Whisper for robotic applications"}),"\n",(0,s.jsx)(r.li,{children:"Integrate Whisper with ROS 2 for real-time voice processing"}),"\n",(0,s.jsx)(r.li,{children:"Process voice commands and convert them to text"}),"\n",(0,s.jsx)(r.li,{children:"Handle various acoustic conditions and noise environments"}),"\n",(0,s.jsx)(r.li,{children:"Implement voice activity detection and command parsing"}),"\n",(0,s.jsx)(r.li,{children:"Validate voice processing accuracy and latency"}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"71-introduction-to-whisper-for-robotics",children:"7.1 Introduction to Whisper for Robotics"}),"\n",(0,s.jsx)(r.h3,{id:"what-is-whisper",children:"What is Whisper?"}),"\n",(0,s.jsx)(r.p,{children:"Whisper is a general-purpose speech recognition model developed by OpenAI. It is designed to be robust across various domains, languages, and acoustic conditions. For robotics applications, Whisper provides:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsx)(r.li,{children:"High accuracy speech-to-text conversion"}),"\n",(0,s.jsx)(r.li,{children:"Support for multiple languages"}),"\n",(0,s.jsx)(r.li,{children:"Robustness to background noise"}),"\n",(0,s.jsx)(r.li,{children:"Real-time processing capabilities"}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"key-features-for-robotics",children:"Key Features for Robotics"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Multi-language Support"}),": Supports 99 languages for international applications"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Robustness"}),": Performs well in noisy environments"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Timestamps"}),": Provides word-level timing information"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Punctuation"}),": Automatically adds punctuation to transcriptions"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Speaker Identification"}),": Can distinguish between different speakers"]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"whisper-model-variants",children:"Whisper Model Variants"}),"\n",(0,s.jsx)(r.p,{children:"Whisper comes in several sizes optimized for different performance requirements:"}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Model"}),(0,s.jsx)(r.th,{children:"Parameters"}),(0,s.jsx)(r.th,{children:"Required VRAM"}),(0,s.jsx)(r.th,{children:"Relative Speed"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"tiny"}),(0,s.jsx)(r.td,{children:"39 M"}),(0,s.jsx)(r.td,{children:"~1 GB"}),(0,s.jsx)(r.td,{children:"32x"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"base"}),(0,s.jsx)(r.td,{children:"74 M"}),(0,s.jsx)(r.td,{children:"~1 GB"}),(0,s.jsx)(r.td,{children:"16x"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"small"}),(0,s.jsx)(r.td,{children:"244 M"}),(0,s.jsx)(r.td,{children:"~2 GB"}),(0,s.jsx)(r.td,{children:"6x"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"medium"}),(0,s.jsx)(r.td,{children:"769 M"}),(0,s.jsx)(r.td,{children:"~5 GB"}),(0,s.jsx)(r.td,{children:"2x"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"large"}),(0,s.jsx)(r.td,{children:"1550 M"}),(0,s.jsx)(r.td,{children:"~10 GB"}),(0,s.jsx)(r.td,{children:"1x"})]})]})]}),"\n",(0,s.jsx)(r.p,{children:'For robotics applications, the "small" or "medium" models typically provide the best balance of accuracy and computational requirements.'}),"\n",(0,s.jsx)(r.h2,{id:"72-installing-and-setting-up-whisper",children:"7.2 Installing and Setting Up Whisper"}),"\n",(0,s.jsx)(r.h3,{id:"system-requirements",children:"System Requirements"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsx)(r.li,{children:"Python 3.9 or higher"}),"\n",(0,s.jsx)(r.li,{children:"PyTorch 1.10 or higher"}),"\n",(0,s.jsx)(r.li,{children:"At least 2GB RAM (for small model) or 5GB (for medium model)"}),"\n",(0,s.jsx)(r.li,{children:"CUDA-compatible GPU recommended for real-time performance"}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"installation-process",children:"Installation Process"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-bash",children:"# Install Whisper and its dependencies\r\npip install openai-whisper\r\n\r\n# For GPU acceleration (if available)\r\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\r\n\r\n# Additional dependencies for audio processing\r\npip install pyaudio soundfile librosa\n"})}),"\n",(0,s.jsx)(r.h3,{id:"basic-whisper-usage-example",children:"Basic Whisper Usage Example"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'import whisper\r\n\r\n# Load model (downloads automatically on first use)\r\nmodel = whisper.load_model("small")\r\n\r\n# Transcribe audio file\r\nresult = model.transcribe("audio_file.wav")\r\n\r\n# Print the transcribed text\r\nprint(result["text"])\n'})}),"\n",(0,s.jsx)(r.h2,{id:"73-whisper-integration-with-ros-2",children:"7.3 Whisper Integration with ROS 2"}),"\n",(0,s.jsx)(r.h3,{id:"audio-input-from-microphone",children:"Audio Input from Microphone"}),"\n",(0,s.jsx)(r.p,{children:"For real-time voice processing, we need to capture audio from a microphone and process it in chunks:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nimport pyaudio\r\nimport numpy as np\r\nimport whisper\r\nimport threading\r\nfrom std_msgs.msg import String\r\nfrom audio_common_msgs.msg import AudioData\r\n\r\nclass WhisperNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'whisper_node\')\r\n\r\n        # Initialize Whisper model\r\n        self.model = whisper.load_model("small", device="cuda" if torch.cuda.is_available() else "cpu")\r\n\r\n        # Audio parameters\r\n        self.rate = 16000  # Sample rate\r\n        self.chunk = 1024  # Buffer size\r\n        self.format = pyaudio.paInt16\r\n        self.channels = 1\r\n\r\n        # Initialize PyAudio\r\n        self.audio = pyaudio.PyAudio()\r\n\r\n        # ROS 2 publishers\r\n        self.transcript_pub = self.create_publisher(String, \'voice_transcript\', 10)\r\n        self.command_pub = self.create_publisher(String, \'voice_command\', 10)\r\n\r\n        # Start audio stream\r\n        self.stream = self.audio.open(\r\n            format=self.format,\r\n            channels=self.channels,\r\n            rate=self.rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk\r\n        )\r\n\r\n        # Start processing thread\r\n        self.processing_thread = threading.Thread(target=self.process_audio)\r\n        self.processing_thread.daemon = True\r\n        self.processing_thread.start()\r\n\r\n        self.get_logger().info(\'Whisper node initialized\')\r\n\r\n    def process_audio(self):\r\n        """Process audio in chunks for real-time transcription"""\r\n        buffer = np.array([], dtype=np.float32)\r\n\r\n        while rclpy.ok():\r\n            # Read audio data\r\n            data = self.stream.read(self.chunk, exception_on_overflow=False)\r\n            audio_data = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\r\n\r\n            # Add to buffer\r\n            buffer = np.concatenate([buffer, audio_data])\r\n\r\n            # Process when buffer reaches 5 seconds (80,000 samples at 16kHz)\r\n            if len(buffer) >= self.rate * 5:\r\n                # Process audio with Whisper\r\n                transcript = self.transcribe_audio(buffer)\r\n\r\n                if transcript.strip():  # Only publish if there\'s actual text\r\n                    self.publish_transcript(transcript)\r\n\r\n                # Keep last 1 second of audio to maintain context\r\n                buffer = buffer[-self.rate:]\r\n\r\n    def transcribe_audio(self, audio_buffer):\r\n        """Transcribe audio buffer using Whisper"""\r\n        try:\r\n            # Convert to the format expected by Whisper\r\n            audio_tensor = torch.from_numpy(audio_buffer).to(self.model.device)\r\n\r\n            # Transcribe\r\n            result = self.model.transcribe(audio_tensor.cpu().numpy())\r\n            return result["text"]\r\n        except Exception as e:\r\n            self.get_logger().error(f\'Error during transcription: {e}\')\r\n            return ""\r\n\r\n    def publish_transcript(self, transcript):\r\n        """Publish transcript to ROS topics"""\r\n        # Publish raw transcript\r\n        transcript_msg = String()\r\n        transcript_msg.data = transcript\r\n        self.transcript_pub.publish(transcript_msg)\r\n\r\n        # Extract and publish commands (simplified - in practice you\'d use NLP)\r\n        command = self.extract_command(transcript)\r\n        if command:\r\n            command_msg = String()\r\n            command_msg.data = command\r\n            self.command_pub.publish(command_msg)\r\n\r\n    def extract_command(self, transcript):\r\n        """Simple command extraction (in practice, use more sophisticated NLP)"""\r\n        # Convert to lowercase for easier matching\r\n        text = transcript.lower().strip()\r\n\r\n        # Define simple command patterns\r\n        commands = [\r\n            "move forward", "move backward", "turn left", "turn right",\r\n            "stop", "sit down", "stand up", "wave", "dance", "hello"\r\n        ]\r\n\r\n        for cmd in commands:\r\n            if cmd in text:\r\n                return cmd\r\n\r\n        return None\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    whisper_node = WhisperNode()\r\n\r\n    try:\r\n        rclpy.spin(whisper_node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        whisper_node.stream.stop_stream()\r\n        whisper_node.stream.close()\r\n        whisper_node.audio.terminate()\r\n        whisper_node.destroy_node()\r\n        rclpy.shutdown()\n'})}),"\n",(0,s.jsx)(r.h2,{id:"74-voice-activity-detection",children:"7.4 Voice Activity Detection"}),"\n",(0,s.jsx)(r.h3,{id:"importance-in-robotics",children:"Importance in Robotics"}),"\n",(0,s.jsx)(r.p,{children:"Voice activity detection (VAD) is crucial for robotics applications to:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsx)(r.li,{children:"Reduce computational load by only processing when speech is detected"}),"\n",(0,s.jsx)(r.li,{children:"Improve accuracy by avoiding background noise processing"}),"\n",(0,s.jsx)(r.li,{children:"Enable more responsive voice interaction"}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"implementation-with-silero-vad",children:"Implementation with Silero VAD"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'import torch\r\nimport torchaudio\r\nfrom scipy import signal\r\n\r\nclass VoiceActivityDetector:\r\n    def __init__(self, threshold=0.5):\r\n        # Load Silero VAD model\r\n        self.model, _ = torch.hub.load(\r\n            repo_or_dir=\'snakers4/silero-vad\',\r\n            model=\'silero_vad\',\r\n            force_reload=False\r\n        )\r\n        self.threshold = threshold\r\n        self.sample_rate = 16000\r\n\r\n    def is_speech(self, audio_chunk):\r\n        """Detect if audio chunk contains speech"""\r\n        # Ensure audio is in the right format\r\n        if len(audio_chunk.shape) == 1:\r\n            audio_chunk = audio_chunk.unsqueeze(0)\r\n\r\n        # Get VAD probability\r\n        vad_prob = self.model(audio_chunk, self.sample_rate).item()\r\n\r\n        return vad_prob > self.threshold\n'})}),"\n",(0,s.jsx)(r.h3,{id:"integration-with-whisper-processing",children:"Integration with Whisper Processing"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'class WhisperWithVAD(WhisperNode):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        # Initialize VAD\r\n        self.vad = VoiceActivityDetector()\r\n\r\n        # Voice activity state\r\n        self.is_listening = False\r\n        self.voice_buffer = np.array([], dtype=np.float32)\r\n        self.silence_counter = 0\r\n        self.min_voice_duration = self.rate * 0.5  # Minimum 0.5 seconds of voice\r\n        self.max_silence_duration = self.rate * 1.0  # Maximum 1 second of silence\r\n\r\n    def process_audio(self):\r\n        """Process audio with VAD for more efficient processing"""\r\n        while rclpy.ok():\r\n            # Read audio data\r\n            data = self.stream.read(self.chunk, exception_on_overflow=False)\r\n            audio_data = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\r\n\r\n            # Check for voice activity\r\n            is_speech = self.vad.is_speech(torch.from_numpy(audio_data))\r\n\r\n            if is_speech:\r\n                # Add to voice buffer\r\n                self.voice_buffer = np.concatenate([self.voice_buffer, audio_data])\r\n                self.silence_counter = 0\r\n                self.is_listening = True\r\n            elif self.is_listening:\r\n                # We were listening but now there\'s silence\r\n                self.silence_counter += len(audio_data)\r\n\r\n                # If enough silence, process the collected voice\r\n                if self.silence_counter >= self.max_silence_duration or len(self.voice_buffer) >= self.rate * 5:\r\n                    if len(self.voice_buffer) >= self.min_voice_duration:\r\n                        # Process collected voice segment\r\n                        transcript = self.transcribe_audio(self.voice_buffer)\r\n                        if transcript.strip():\r\n                            self.publish_transcript(transcript)\r\n\r\n                    # Reset for next voice segment\r\n                    self.voice_buffer = np.array([], dtype=np.float32)\r\n                    self.is_listening = False\r\n                    self.silence_counter = 0\n'})}),"\n",(0,s.jsx)(r.h2,{id:"75-real-time-processing-optimization",children:"7.5 Real-time Processing Optimization"}),"\n",(0,s.jsx)(r.h3,{id:"batch-processing",children:"Batch Processing"}),"\n",(0,s.jsx)(r.p,{children:"For improved efficiency, process multiple audio chunks together:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'class OptimizedWhisperNode(WhisperNode):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        # Processing parameters\r\n        self.processing_interval = 2.0  # Process every 2 seconds\r\n        self.buffer_size = int(self.rate * self.processing_interval)\r\n        self.audio_buffer = np.array([], dtype=np.float32)\r\n\r\n        # Timer for periodic processing\r\n        self.process_timer = self.create_timer(\r\n            self.processing_interval,\r\n            self.process_buffered_audio\r\n        )\r\n\r\n    def process_buffered_audio(self):\r\n        """Process accumulated audio buffer"""\r\n        if len(self.audio_buffer) > 0:\r\n            # Process the accumulated buffer\r\n            transcript = self.transcribe_audio(self.audio_buffer)\r\n\r\n            if transcript.strip():\r\n                self.publish_transcript(transcript)\r\n\r\n            # Clear buffer\r\n            self.audio_buffer = np.array([], dtype=np.float32)\r\n\r\n    def process_audio(self):\r\n        """Accumulate audio for batch processing"""\r\n        while rclpy.ok():\r\n            data = self.stream.read(self.chunk, exception_on_overflow=False)\r\n            audio_data = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\r\n\r\n            # Add to buffer\r\n            self.audio_buffer = np.concatenate([self.audio_buffer, audio_data])\r\n\r\n            # Keep buffer size reasonable\r\n            if len(self.audio_buffer) > self.buffer_size * 2:\r\n                self.audio_buffer = self.audio_buffer[-self.buffer_size:]\n'})}),"\n",(0,s.jsx)(r.h2,{id:"76-handling-acoustic-conditions",children:"7.6 Handling Acoustic Conditions"}),"\n",(0,s.jsx)(r.h3,{id:"noise-reduction",children:"Noise Reduction"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'from scipy import signal\r\nimport webrtcvad\r\n\r\nclass RobustWhisperNode(WhisperNode):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        # Initialize noise reduction\r\n        self.setup_noise_reduction()\r\n\r\n    def setup_noise_reduction(self):\r\n        """Set up noise reduction filters"""\r\n        # Create a simple low-pass filter to remove high-frequency noise\r\n        nyquist = self.rate / 2\r\n        cutoff = 8000  # Hz\r\n        order = 6\r\n        b, a = signal.butter(order, cutoff / nyquist, btype=\'low\')\r\n        self.filter_b = b\r\n        self.filter_a = a\r\n\r\n    def preprocess_audio(self, audio_data):\r\n        """Apply noise reduction and preprocessing"""\r\n        # Apply low-pass filter\r\n        filtered_audio = signal.filtfilt(self.filter_b, self.filter_a, audio_data)\r\n\r\n        # Normalize audio\r\n        max_val = np.max(np.abs(filtered_audio))\r\n        if max_val > 0:\r\n            filtered_audio = filtered_audio / max_val\r\n\r\n        return filtered_audio\r\n\r\n    def process_audio(self):\r\n        """Process audio with preprocessing"""\r\n        buffer = np.array([], dtype=np.float32)\r\n\r\n        while rclpy.ok():\r\n            data = self.stream.read(self.chunk, exception_on_overflow=False)\r\n            audio_data = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0\r\n\r\n            # Apply preprocessing\r\n            processed_audio = self.preprocess_audio(audio_data)\r\n\r\n            # Add to buffer\r\n            buffer = np.concatenate([buffer, processed_audio])\r\n\r\n            # Process when buffer reaches 5 seconds\r\n            if len(buffer) >= self.rate * 5:\r\n                transcript = self.transcribe_audio(buffer)\r\n\r\n                if transcript.strip():\r\n                    self.publish_transcript(transcript)\r\n\r\n                # Keep last 1 second\r\n                buffer = buffer[-self.rate:]\n'})}),"\n",(0,s.jsx)(r.h2,{id:"77-voice-command-parsing",children:"7.7 Voice Command Parsing"}),"\n",(0,s.jsx)(r.h3,{id:"simple-command-recognition",children:"Simple Command Recognition"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'import re\r\nfrom dataclasses import dataclass\r\nfrom typing import Optional\r\n\r\n@dataclass\r\nclass VoiceCommand:\r\n    action: str\r\n    parameters: dict\r\n    confidence: float\r\n\r\nclass VoiceCommandParser:\r\n    def __init__(self):\r\n        # Define command patterns\r\n        self.command_patterns = [\r\n            {\r\n                "pattern": r"go\\s+(forward|backward|left|right|up|down)",\r\n                "action": "move",\r\n                "extractor": lambda match: {"direction": match.group(1)}\r\n            },\r\n            {\r\n                "pattern": r"move\\s+(forward|backward|left|right|up|down)",\r\n                "action": "move",\r\n                "extractor": lambda match: {"direction": match.group(1)}\r\n            },\r\n            {\r\n                "pattern": r"turn\\s+(left|right)",\r\n                "action": "turn",\r\n                "extractor": lambda match: {"direction": match.group(1)}\r\n            },\r\n            {\r\n                "pattern": r"go\\s+to\\s+(.+)",\r\n                "action": "navigate",\r\n                "extractor": lambda match: {"location": match.group(1).strip()}\r\n            },\r\n            {\r\n                "pattern": r"pick\\s+up\\s+(.+)",\r\n                "action": "pick",\r\n                "extractor": lambda match: {"object": match.group(1).strip()}\r\n            },\r\n            {\r\n                "pattern": r"put\\s+down|place",\r\n                "action": "place",\r\n                "extractor": lambda match: {}\r\n            },\r\n            {\r\n                "pattern": r"stop|halt|freeze",\r\n                "action": "stop",\r\n                "extractor": lambda match: {}\r\n            }\r\n        ]\r\n\r\n    def parse_command(self, transcript: str) -> Optional[VoiceCommand]:\r\n        """Parse voice transcript into structured command"""\r\n        transcript_lower = transcript.lower().strip()\r\n\r\n        for pattern_info in self.command_patterns:\r\n            match = re.search(pattern_info["pattern"], transcript_lower)\r\n            if match:\r\n                try:\r\n                    parameters = pattern_info["extractor"](match)\r\n                    return VoiceCommand(\r\n                        action=pattern_info["action"],\r\n                        parameters=parameters,\r\n                        confidence=0.8  # Simple confidence for now\r\n                    )\r\n                except Exception:\r\n                    continue\r\n\r\n        # If no specific pattern matched, return a general command\r\n        return VoiceCommand(\r\n            action="unknown",\r\n            parameters={"text": transcript},\r\n            confidence=0.5\r\n        )\n'})}),"\n",(0,s.jsx)(r.h2,{id:"78-practical-exercise-voice-processing-system",children:"7.8 Practical Exercise: Voice Processing System"}),"\n",(0,s.jsx)(r.h3,{id:"exercise-objective",children:"Exercise Objective"}),"\n",(0,s.jsx)(r.p,{children:"Create a complete voice processing system that integrates Whisper with ROS 2 for robotic command recognition."}),"\n",(0,s.jsx)(r.h3,{id:"steps",children:"Steps"}),"\n",(0,s.jsxs)(r.ol,{children:["\n",(0,s.jsx)(r.li,{children:"Set up Whisper model with appropriate size for your hardware"}),"\n",(0,s.jsx)(r.li,{children:"Implement real-time audio capture and processing"}),"\n",(0,s.jsx)(r.li,{children:"Add voice activity detection to optimize processing"}),"\n",(0,s.jsx)(r.li,{children:"Implement command parsing for robotic actions"}),"\n",(0,s.jsx)(r.li,{children:"Test the system with various voice commands"}),"\n",(0,s.jsx)(r.li,{children:"Validate accuracy and response time"}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"expected-results",children:"Expected Results"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:["Real-time voice processing with ",(0,s.jsx)(r.code,{children:"<2"})," second latency"]}),"\n",(0,s.jsx)(r.li,{children:"Accurate transcription (>80% accuracy in quiet conditions)"}),"\n",(0,s.jsx)(r.li,{children:"Proper command recognition and parsing"}),"\n",(0,s.jsx)(r.li,{children:"Integration with ROS 2 messaging system"}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(r.p,{children:"Voice processing with Whisper provides a robust foundation for natural human-robot interaction. By properly integrating Whisper with ROS 2 and implementing voice activity detection, we can create responsive voice interfaces for humanoid robots. The key is balancing accuracy, latency, and computational efficiency for real-world robotic applications."}),"\n",(0,s.jsx)(r.h2,{id:"key-terms",children:"Key Terms"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Whisper"}),": OpenAI's automatic speech recognition system"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Voice Activity Detection (VAD)"}),": System to detect when speech is present"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Real-time Processing"}),": Processing audio as it's captured"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Acoustic Conditions"}),": Environmental factors affecting audio quality"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Command Parsing"}),": Converting natural language to structured commands"]}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"references",children:"References"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsx)(r.li,{children:(0,s.jsx)(r.a,{href:"https://github.com/openai/whisper",children:"OpenAI Whisper GitHub"})}),"\n",(0,s.jsx)(r.li,{children:(0,s.jsx)(r.a,{href:"https://github.com/snakers4/silero-vad",children:"Silero VAD"})}),"\n",(0,s.jsx)(r.li,{children:(0,s.jsx)(r.a,{href:"https://pyaudio.readthedocs.io/",children:"PyAudio Documentation"})}),"\n",(0,s.jsx)(r.li,{children:(0,s.jsx)(r.a,{href:"https://arxiv.org/abs/2104.02744",children:"Speech Recognition in Robotics"})}),"\n"]})]})}function p(e={}){const{wrapper:r}={...(0,t.R)(),...e.components};return r?(0,s.jsx)(r,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);