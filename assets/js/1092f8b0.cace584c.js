"use strict";(globalThis.webpackChunkai_robotics_textbook=globalThis.webpackChunkai_robotics_textbook||[]).push([[4633],{4705:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>t,contentTitle:()=>o,default:()=>d,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"assets/references/vla-references","title":"Vision-Language-Action Integration References","description":"Vision-Language-Action Research","source":"@site/docs/assets/references/vla-references.md","sourceDirName":"assets/references","slug":"/assets/references/vla-references","permalink":"/ai-robotics-textbook/docs/assets/references/vla-references","draft":false,"unlisted":false,"editUrl":"https://github.com/humonide-book/ai-robotics-textbook/edit/main/docs/assets/references/vla-references.md","tags":[],"version":"current","frontMatter":{}}');var i=r(4848),a=r(8453);const l={},o="Vision-Language-Action Integration References",t={},c=[{value:"Vision-Language-Action Research",id:"vision-language-action-research",level:2},{value:"Large Language Models in Robotics",id:"large-language-models-in-robotics",level:2},{value:"Voice Processing and Speech Recognition",id:"voice-processing-and-speech-recognition",level:2},{value:"Multi-Modal Integration",id:"multi-modal-integration",level:2},{value:"Robotics and AI Integration",id:"robotics-and-ai-integration",level:2},{value:"Human-Robot Interaction",id:"human-robot-interaction",level:2},{value:"Safety and Validation",id:"safety-and-validation",level:2},{value:"Open-Source Tools and Frameworks",id:"open-source-tools-and-frameworks",level:2},{value:"Recent Advances in VLA Systems",id:"recent-advances-in-vla-systems",level:2},{value:"Datasets and Benchmarks",id:"datasets-and-benchmarks",level:2}];function h(n){const e={a:"a",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.header,{children:(0,i.jsx)(e.h1,{id:"vision-language-action-integration-references",children:"Vision-Language-Action Integration References"})}),"\n",(0,i.jsx)(e.h2,{id:"vision-language-action-research",children:"Vision-Language-Action Research"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Liang, J., et al. (2024)."}),"\r\n",(0,i.jsx)(e.em,{children:"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robot Manipulation."}),"\r\narXiv preprint arXiv:2307.15818."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2307.15818",children:"https://arxiv.org/abs/2307.15818"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Brohan, C., et al. (2024)."}),"\r\n",(0,i.jsx)(e.em,{children:"Q-Transformer: Scalable Robot Learning with Masked Tokens and Reinforcement Learning."}),"\r\narXiv preprint arXiv:2403.19434."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2403.19434",children:"https://arxiv.org/abs/2403.19434"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Kaplan, A., et al. (2023)."}),"\r\n",(0,i.jsx)(e.em,{children:"A Generalist Robot Learning Model via Bootstrapped Imitation Learning from Language."}),"\r\narXiv preprint arXiv:2305.13953."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2305.13953",children:"https://arxiv.org/abs/2305.13953"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Nair, A. V., et al. (2022)."}),"\r\n",(0,i.jsx)(e.em,{children:"Collaboration through imitation: Robotic manipulation utilizing large language models and human demonstrations."}),"\r\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"DOI: 10.1109/IROS47612.2022.9981914"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"large-language-models-in-robotics",children:"Large Language Models in Robotics"}),"\n",(0,i.jsxs)(e.ol,{start:"5",children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Huang, S., et al. (2023)."}),"\r\n",(0,i.jsx)(e.em,{children:"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents."}),"\r\nInternational Conference on Machine Learning (ICML)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2208.01115",children:"https://arxiv.org/abs/2208.01115"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Brohan, C., et al. (2023)."}),"\r\n",(0,i.jsx)(e.em,{children:"RT-1: Robotics Transformer for Real-World Control at Scale."}),"\r\nConference on Robot Learning (CoRL)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2212.06817",children:"https://arxiv.org/abs/2212.06817"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Ahn, M., et al. (2022)."}),"\r\n",(0,i.jsx)(e.em,{children:"Do as I Can, Not as I Say: Grounding Language in Robotic Affordances."}),"\r\nConference on Robot Learning (CoRL)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2204.01691",children:"https://arxiv.org/abs/2204.01691"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Huang, W., et al. (2022)."}),"\r\n",(0,i.jsx)(e.em,{children:"Language as Grounds for Interaction with the World."}),"\r\narXiv preprint arXiv:2206.05442."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2206.05442",children:"https://arxiv.org/abs/2206.05442"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"voice-processing-and-speech-recognition",children:"Voice Processing and Speech Recognition"}),"\n",(0,i.jsxs)(e.ol,{start:"9",children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Radford, A., et al. (2022)."}),"\r\n",(0,i.jsx)(e.em,{children:"Robust Speech Recognition via Large-Scale Weak Supervision."}),"\r\nInternational Conference on Machine Learning (ICML)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2212.04356",children:"https://arxiv.org/abs/2212.04356"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Klejch, O., et al. (2021)."}),"\r\n",(0,i.jsx)(e.em,{children:"End-to-End Trainable Voice-Controlled Robotic Assistant."}),"\r\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"DOI: 10.1109/ICASSP36960.2021.9414631"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Moritz, D., et al. (2022)."}),"\r\n",(0,i.jsx)(e.em,{children:"Voice-Operated Mobile Manipulation."}),"\r\nIEEE International Conference on Robotics and Automation (ICRA)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"DOI: 10.1109/ICRA46639.2022.9811762"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Kollar, T., et al. (2009)."}),"\r\n",(0,i.jsx)(e.em,{children:"Toward understanding natural language spatial references in human-robot interaction."}),"\r\nACM/IEEE International Conference on Human-Robot Interaction (HRI)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"DOI: 10.1145/1514095.1514108"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"multi-modal-integration",children:"Multi-Modal Integration"}),"\n",(0,i.jsxs)(e.ol,{start:"13",children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Lu, J., et al. (2019)."}),"\r\n",(0,i.jsx)(e.em,{children:"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks."}),"\r\nAdvances in Neural Information Processing Systems (NeurIPS)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/1908.02265",children:"https://arxiv.org/abs/1908.02265"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Chen, X., et al. (2020)."}),"\r\n",(0,i.jsx)(e.em,{children:"UNITER: UNiversal Image-TExt Representation Learning."}),"\r\nEuropean Conference on Computer Vision (ECCV)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/1909.11740",children:"https://arxiv.org/abs/1909.11740"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Li, L., et al. (2020)."}),"\r\n",(0,i.jsx)(e.em,{children:"Unicoder-VL: A Vision-Language BERT for Unifying Cross-Modal and Within-Modal Tasks."}),"\r\narXiv preprint arXiv:2003.11543."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2003.11543",children:"https://arxiv.org/abs/2003.11543"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Tan, H., & Bansal, M. (2019)."}),"\r\n",(0,i.jsx)(e.em,{children:"LXMERT: Learning Cross-Modality Encoder Representations from Transformers."}),"\r\nConference on Empirical Methods in Natural Language Processing (EMNLP)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/1908.07490",children:"https://arxiv.org/abs/1908.07490"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"robotics-and-ai-integration",children:"Robotics and AI Integration"}),"\n",(0,i.jsxs)(e.ol,{start:"17",children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Kober, J., Bagnell, J. A., & Peters, J. (2013)."}),"\r\n",(0,i.jsx)(e.em,{children:"Reinforcement learning in robotics: A survey."}),"\r\nThe International Journal of Robotics Research, 32(11), 1238-1274."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"DOI: 10.1177/0278364913495721"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Saxena, A., et al. (2008)."}),"\r\n",(0,i.jsx)(e.em,{children:"Robot grasp detection using convolutional neural networks."}),"\r\nIEEE International Conference on Robotics and Automation (ICRA)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"DOI: 10.1109/ICRA.2008.4549222"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Pinto, L., & Gupta, A. (2016)."}),"\r\n",(0,i.jsx)(e.em,{children:"Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours."}),"\r\nIEEE International Conference on Robotics and Automation (ICRA)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"DOI: 10.1109/ICRA.2016.7487419"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Zeng, A., et al. (2018)."}),"\r\n",(0,i.jsx)(e.em,{children:"Learning synergies between pushing and grasping with self-supervised deep reinforcement learning."}),"\r\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"DOI: 10.1109/IROS.2018.8593582"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"human-robot-interaction",children:"Human-Robot Interaction"}),"\n",(0,i.jsxs)(e.ol,{start:"21",children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Breazeal, C. (2003)."}),"\r\n",(0,i.jsx)(e.em,{children:"Toward sociable robots."}),"\r\nRobotics and Autonomous Systems, 42(3-4), 167-175."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"DOI: 10.1016/S0921-8890(02)00373-X"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Mutlu, B., & Forlizzi, J. (2008)."}),"\r\n",(0,i.jsx)(e.em,{children:"Roles for robots: a taxonomy for human-robot interaction."}),"\r\nRO-MAN 2008 - The 17th IEEE International Symposium on Robot and Human Interactive Communication."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"DOI: 10.1109/ROMAN.2008.4600702"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Tapus, A., et al. (2007)."}),"\r\n",(0,i.jsx)(e.em,{children:"User acceptance of care robot companion in the household: Results from a long-term field study."}),"\r\nIEEE International Conference on Rehabilitation Robotics."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"DOI: 10.1109/ICORR.2007.4428524"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Fong, T., Nourbakhsh, I., & Dautenhahn, K. (2003)."}),"\r\n",(0,i.jsx)(e.em,{children:"A survey of socially interactive robots."}),"\r\nRobotics and Autonomous Systems, 42(3-4), 143-166."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"DOI: 10.1016/S0921-8890(02)00372-8"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"safety-and-validation",children:"Safety and Validation"}),"\n",(0,i.jsxs)(e.ol,{start:"25",children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"ISO 13482:2014."}),"\r\n",(0,i.jsx)(e.em,{children:"Robots and robotic devices \u2014 Safety requirements for personal care robots."}),"\r\nInternational Organization for Standardization."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://www.iso.org/standard/45144.html",children:"https://www.iso.org/standard/45144.html"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"ISO 10218-1:2011."}),"\r\n",(0,i.jsx)(e.em,{children:"Robots and robotic devices \u2014 Safety requirements for industrial robots \u2014 Part 1: Robots."}),"\r\nInternational Organization for Standardization."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://www.iso.org/standard/49427.html",children:"https://www.iso.org/standard/49427.html"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Schaft, A. J. V. D., & Stramigioli, S. (2017)."}),"\r\n",(0,i.jsx)(e.em,{children:"Modeling and Control of Robots."}),"\r\nIEEE Control Systems Magazine, 37(1), 17-28."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"DOI: 10.1109/MCS.2016.2626363"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Amodei, D., et al. (2016)."}),"\r\n",(0,i.jsx)(e.em,{children:"Concrete problems in AI safety."}),"\r\narXiv preprint arXiv:1606.06565."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/1606.06565",children:"https://arxiv.org/abs/1606.06565"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"open-source-tools-and-frameworks",children:"Open-Source Tools and Frameworks"}),"\n",(0,i.jsxs)(e.ol,{start:"29",children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Quigley, M., et al. (2009)."}),"\r\n",(0,i.jsx)(e.em,{children:"ROS: an open-source robot operating system."}),"\r\nICRA Workshop on Open Source Software, Kobe, Japan."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"http://www.ros.org/",children:"http://www.ros.org/"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"OpenAI. (2022)."}),"\r\n",(0,i.jsx)(e.em,{children:"Whisper: Robust Speech Recognition via Large-Scale Weak Supervision."})]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://github.com/openai/whisper",children:"https://github.com/openai/whisper"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Brown, T., et al. (2020)."}),"\r\n",(0,i.jsx)(e.em,{children:"Language Models are Few-Shot Learners."}),"\r\nAdvances in Neural Information Processing Systems (NeurIPS)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2005.14165",children:"https://arxiv.org/abs/2005.14165"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Ramesh, A., et al. (2022)."}),"\r\n",(0,i.jsx)(e.em,{children:"Hierarchical Text-Conditional Image Generation with CLIP Latents."}),"\r\narXiv preprint arXiv:2204.06125."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://github.com/openai/DALL-E",children:"https://github.com/openai/DALL-E"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"recent-advances-in-vla-systems",children:"Recent Advances in VLA Systems"}),"\n",(0,i.jsxs)(e.ol,{start:"33",children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Garg, S., et al. (2024)."}),"\r\n",(0,i.jsx)(e.em,{children:"VoxPoser: Compositional 3D Value Maps for Robotic Manipulation with Language Models."}),"\r\narXiv preprint arXiv:2402.08678."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2402.08678",children:"https://arxiv.org/abs/2402.08678"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Zhou, Y., et al. (2023)."}),"\r\n",(0,i.jsx)(e.em,{children:"Open-Vocabulary Object Detection via Language-Guided Hierarchical Visual Mapping."}),"\r\narXiv preprint arXiv:2308.09713."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2308.09713",children:"https://arxiv.org/abs/2308.09713"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Shridhar, M., et al. (2022)."}),"\r\n",(0,i.jsx)(e.em,{children:"Cliport: What and where pathways for robotic manipulation."}),"\r\nConference on Robot Learning (CoRL)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2109.12098",children:"https://arxiv.org/abs/2109.12098"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Fan, L., et al. (2021)."}),"\r\n",(0,i.jsx)(e.em,{children:"Memorizing Transformers: Leveraging Long-Range Episodic Memory for Multitask Control."}),"\r\nConference on Robot Learning (CoRL)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://arxiv.org/abs/2107.05645",children:"https://arxiv.org/abs/2107.05645"})]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"datasets-and-benchmarks",children:"Datasets and Benchmarks"}),"\n",(0,i.jsxs)(e.ol,{start:"37",children:["\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Tellex, S., et al. (2011)."}),"\r\n",(0,i.jsx)(e.em,{children:"Understanding natural language commands for robotic navigation."}),"\r\nAAAI Conference on Artificial Intelligence."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:["URL: ",(0,i.jsx)(e.a,{href:"https://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/view/3715",children:"https://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/view/3715"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Misra, D., et al. (2018)."}),"\r\n",(0,i.jsx)(e.em,{children:"Mapping instructions and visual observations to actions with reinforcement learning."}),"\r\nRobotics: Science and Systems (RSS)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"DOI: 10.15607/RSS.2018.XIV.043"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Herbst, E., et al. (2013)."}),"\r\n",(0,i.jsx)(e.em,{children:"Real-time joint tracking of a human worker for collaboration in a shared workspace."}),"\r\nInternational Conference on Intelligent Robots and Systems (IROS)."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"DOI: 10.1109/IROS.2013.6696984"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(e.li,{children:["\n",(0,i.jsxs)(e.p,{children:[(0,i.jsx)(e.strong,{children:"Zeng, A., et al. (2019)."}),"\r\n",(0,i.jsx)(e.em,{children:"Learning Dexterous In-Hand Manipulation."}),"\r\nThe International Journal of Robotics Research, 39(2-3), 388-404."]}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"DOI: 10.1177/0278364919887178"}),"\n"]}),"\n"]}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(h,{...n})}):h(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>l,x:()=>o});var s=r(6540);const i={},a=s.createContext(i);function l(n){const e=s.useContext(a);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(i):n.components||i:l(n.components),s.createElement(a.Provider,{value:e},n.children)}}}]);